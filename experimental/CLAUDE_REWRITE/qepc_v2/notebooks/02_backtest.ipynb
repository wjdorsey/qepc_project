{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ QEPC Backtest\n",
    "\n",
    "**Validate predictions against actual results**\n",
    "\n",
    "This notebook:\n",
    "1. Runs time-travel backtesting (only uses data available before each game)\n",
    "2. Calculates accuracy metrics\n",
    "3. Analyzes calibration\n",
    "4. Generates visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP - Run this first!\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Point directly to your project root (where the data folder is)\n",
    "project_root = Path(r\"C:\\Users\\wdors\\qepc_project\")\n",
    "\n",
    "# Add the new QEPC v2 code to Python path\n",
    "qepc_v2_path = project_root / \"experimental\" / \"CLAUDE_REWRITE\" / \"qepc_v2\"\n",
    "\n",
    "if str(qepc_v2_path) not in sys.path:\n",
    "    sys.path.insert(0, str(qepc_v2_path))\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üì¶ QEPC v2 code: {qepc_v2_path}\")\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from qepc.data.loader import DataLoader\n",
    "from qepc.sports.nba.backtest import BacktestEngine\n",
    "\n",
    "# Optional: Plotting\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_PLOTS = True\n",
    "    print(\"‚úÖ Matplotlib available\")\n",
    "except ImportError:\n",
    "    HAS_PLOTS = False\n",
    "    print(\"‚ö†Ô∏è  Matplotlib not available - skipping plots\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to backtest!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è Configure Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKTEST CONFIGURATION\n",
    "# Adjust these settings:\n",
    "\n",
    "N_DAYS = 14  # Number of days to backtest\n",
    "\n",
    "print(f\"üìÖ Will backtest last {N_DAYS} days of games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Run Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loader with explicit project root\n",
    "loader = DataLoader(project_root=project_root)\n",
    "\n",
    "# Create backtest engine\n",
    "engine = BacktestEngine(data_loader=loader)\n",
    "\n",
    "# Run backtest\n",
    "summary = engine.run_backtest(\n",
    "    n_days=N_DAYS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results as DataFrame\n",
    "results_df = engine.results_to_dataframe()\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(f\"üìä {len(results_df)} games analyzed\")\n",
    "    print(\"\\nSample results:\")\n",
    "    display(results_df.head(10))\n",
    "else:\n",
    "    print(\"‚ùå No results generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Calibration Analysis\n",
    "\n",
    "Do 60% predictions actually win 60% of the time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration analysis\n",
    "if not results_df.empty:\n",
    "    calibration = engine.calibration_analysis()\n",
    "    \n",
    "    print(\"\\nüéØ CALIBRATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"(Predicted probability vs Actual win rate)\")\n",
    "    print()\n",
    "    display(calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PLOTS and not results_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Spread Error Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(results_df['Spread_Error'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Perfect')\n",
    "    ax1.axvline(results_df['Spread_Error'].mean(), color='orange', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {results_df[\"Spread_Error\"].mean():.1f}')\n",
    "    ax1.set_xlabel('Spread Error (Predicted - Actual)', fontsize=12)\n",
    "    ax1.set_ylabel('Frequency', fontsize=12)\n",
    "    ax1.set_title('Spread Prediction Error Distribution', fontsize=14)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Predicted vs Actual Spread\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.scatter(results_df['Actual_Spread'], results_df['Pred_Spread'], alpha=0.6, s=50)\n",
    "    lims = [min(results_df['Actual_Spread'].min(), results_df['Pred_Spread'].min()) - 5,\n",
    "            max(results_df['Actual_Spread'].max(), results_df['Pred_Spread'].max()) + 5]\n",
    "    ax2.plot(lims, lims, 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    ax2.set_xlabel('Actual Spread', fontsize=12)\n",
    "    ax2.set_ylabel('Predicted Spread', fontsize=12)\n",
    "    ax2.set_title('Predicted vs Actual Spread', fontsize=14)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Accuracy by Confidence\n",
    "    ax3 = axes[1, 0]\n",
    "    results_df['Conf_Bin'] = pd.cut(results_df['Confidence'], \n",
    "                                     bins=[0, 0.4, 0.5, 0.6, 0.7, 1.0],\n",
    "                                     labels=['<40%', '40-50%', '50-60%', '60-70%', '>70%'])\n",
    "    conf_acc = results_df.groupby('Conf_Bin', observed=True)['Winner_Correct'].agg(['mean', 'count'])\n",
    "    \n",
    "    bars = ax3.bar(range(len(conf_acc)), conf_acc['mean'], color='steelblue')\n",
    "    ax3.axhline(0.5, color='red', linestyle='--', linewidth=2, label='50% (Random)')\n",
    "    ax3.set_xticks(range(len(conf_acc)))\n",
    "    ax3.set_xticklabels(conf_acc.index)\n",
    "    ax3.set_xlabel('Model Confidence', fontsize=12)\n",
    "    ax3.set_ylabel('Win Accuracy', fontsize=12)\n",
    "    ax3.set_title('Win Accuracy by Confidence Level', fontsize=14)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, conf_acc['count'])):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'n={int(count)}', ha='center', fontsize=10)\n",
    "    \n",
    "    # 4. Cumulative Accuracy Over Time\n",
    "    ax4 = axes[1, 1]\n",
    "    results_df_sorted = results_df.sort_values('Date')\n",
    "    results_df_sorted['Cumulative_Accuracy'] = results_df_sorted['Winner_Correct'].expanding().mean()\n",
    "    \n",
    "    ax4.plot(range(len(results_df_sorted)), results_df_sorted['Cumulative_Accuracy'], \n",
    "             linewidth=2, color='steelblue')\n",
    "    ax4.axhline(0.5, color='red', linestyle='--', linewidth=2, label='50% (Random)')\n",
    "    ax4.fill_between(range(len(results_df_sorted)), 0.5, results_df_sorted['Cumulative_Accuracy'],\n",
    "                     where=results_df_sorted['Cumulative_Accuracy'] > 0.5, alpha=0.3, color='green')\n",
    "    ax4.fill_between(range(len(results_df_sorted)), 0.5, results_df_sorted['Cumulative_Accuracy'],\n",
    "                     where=results_df_sorted['Cumulative_Accuracy'] < 0.5, alpha=0.3, color='red')\n",
    "    ax4.set_xlabel('Game Number', fontsize=12)\n",
    "    ax4.set_ylabel('Cumulative Accuracy', fontsize=12)\n",
    "    ax4.set_title('Cumulative Win Accuracy Over Time', fontsize=14)\n",
    "    ax4.set_ylim(0.3, 0.8)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations complete\")\n",
    "elif not HAS_PLOTS:\n",
    "    print(\"‚ö†Ô∏è  Install matplotlib for visualizations: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèÜ Best and Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    print(\"\\nüèÜ BEST PREDICTIONS (smallest spread error)\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    results_df['Abs_Error'] = results_df['Spread_Error'].abs()\n",
    "    for _, row in results_df.nsmallest(5, 'Abs_Error').iterrows():\n",
    "        correct = \"‚úÖ\" if row['Winner_Correct'] else \"‚ùå\"\n",
    "        print(f\"   {correct} {row['Away_Team'][:18]:18} @ {row['Home_Team'][:18]:18} | Error: {row['Spread_Error']:+.1f}\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è WORST PREDICTIONS (largest spread error)\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    for _, row in results_df.nlargest(5, 'Abs_Error').iterrows():\n",
    "        correct = \"‚úÖ\" if row['Winner_Correct'] else \"‚ùå\"\n",
    "        print(f\"   {correct} {row['Away_Team'][:18]:18} @ {row['Home_Team'][:18]:18} | Error: {row['Spread_Error']:+.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    output_dir = project_root / 'data' / 'results' / 'backtests'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    filename = f\"QEPC_v2_Backtest_{timestamp}.csv\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Interpretation Guide\n",
    "\n",
    "| Metric | Good | Great | Elite |\n",
    "|--------|------|-------|-------|\n",
    "| Win Accuracy | >52% | >55% | >58% |\n",
    "| Spread MAE | <12 pts | <10 pts | <9 pts |\n",
    "| Brier Score | <0.24 | <0.22 | <0.20 |\n",
    "\n",
    "**Key insights:**\n",
    "- If spread bias is positive ‚Üí model overestimates home team\n",
    "- If high confidence accuracy < overall accuracy ‚Üí model is overconfident\n",
    "- Perfect calibration = predicted probability matches actual win rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
