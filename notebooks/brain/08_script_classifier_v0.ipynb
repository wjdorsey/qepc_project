{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1046c7-cd33-430d-a5fd-8aac0dffca47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[1]\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"qepc in root?\", (PROJECT_ROOT / \"qepc\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e825cfc-30c3-41c1-9272-5e9960693d7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from qepc.brain.games_loader import fetch_league_games, build_games_table\n",
    "from qepc.brain.scripts import label_game_scripts_by_total_points\n",
    "\n",
    "# ðŸ‘‡ switch this to \"2025-26\" later when you want to run current season\n",
    "season = \"2023-24\"\n",
    "\n",
    "team_games = fetch_league_games(season)\n",
    "games_df = build_games_table(team_games)\n",
    "\n",
    "print(\"Season:\", season)\n",
    "print(\"games_df rows:\", len(games_df))\n",
    "display(games_df.head())\n",
    "\n",
    "scripts_df = label_game_scripts_by_total_points(\n",
    "    games_df,\n",
    "    low_quantile=0.25,\n",
    "    high_quantile=0.75,\n",
    ")\n",
    "\n",
    "print(\"scripts_df shape:\", scripts_df.shape)\n",
    "display(scripts_df.head())\n",
    "\n",
    "print(\"\\nScript label distribution:\")\n",
    "print(scripts_df[\"SCRIPT_LABEL\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954ec5a-1648-4098-848e-38e24acd12d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from qepc.brain.teams_loader import fetch_league_team_season_stats\n",
    "\n",
    "# Base counting stats\n",
    "team_stats_base = fetch_league_team_season_stats(\n",
    "    season,\n",
    "    measure_type=\"Base\",\n",
    ")\n",
    "\n",
    "print(\"team_stats_base shape:\", team_stats_base.shape)\n",
    "display(team_stats_base.head())\n",
    "\n",
    "# Advanced stats (OFF_RATING, DEF_RATING, PACE, etc.)\n",
    "team_stats_adv = fetch_league_team_season_stats(\n",
    "    season,\n",
    "    measure_type=\"Advanced\",\n",
    ")\n",
    "\n",
    "print(\"team_stats_adv shape:\", team_stats_adv.shape)\n",
    "display(team_stats_adv.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9151f-23f2-4f87-8098-73e232bf939e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Keep only the columns we care about from advanced stats\n",
    "adv_cols_keep = [\n",
    "    \"TEAM_ID\",\n",
    "    \"TEAM_NAME\",\n",
    "    \"TEAM_ABBREVIATION\",\n",
    "    \"GP\",\n",
    "    \"W\",\n",
    "    \"L\",\n",
    "    \"W_PCT\",\n",
    "    \"MIN\",\n",
    "    \"OFF_RATING\",\n",
    "    \"DEF_RATING\",\n",
    "    \"NET_RATING\",\n",
    "    \"PACE\",\n",
    "    \"PIE\",\n",
    "]\n",
    "\n",
    "adv_cols_keep = [c for c in adv_cols_keep if c in team_stats_adv.columns]\n",
    "team_adv_small = team_stats_adv[adv_cols_keep].copy()\n",
    "\n",
    "print(\"team_adv_small columns:\", list(team_adv_small.columns))\n",
    "display(team_adv_small.head())\n",
    "\n",
    "# Make home/away copies with prefixes\n",
    "home_adv = team_adv_small.add_prefix(\"HOME_\")  # TEAM_ID -> HOME_TEAM_ID\n",
    "away_adv = team_adv_small.add_prefix(\"AWAY_\")  # TEAM_ID -> AWAY_TEAM_ID\n",
    "\n",
    "# Start from games_df\n",
    "games_feat = games_df.copy()\n",
    "\n",
    "# Merge home team stats:\n",
    "games_feat = games_feat.merge(\n",
    "    home_adv,\n",
    "    left_on=\"HOME_TEAM_ID\",   # from games_df\n",
    "    right_on=\"HOME_TEAM_ID\",  # from home_adv (prefixed TEAM_ID)\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Merge away team stats:\n",
    "games_feat = games_feat.merge(\n",
    "    away_adv,\n",
    "    left_on=\"AWAY_TEAM_ID\",   # from games_df\n",
    "    right_on=\"AWAY_TEAM_ID\",  # from away_adv (prefixed TEAM_ID)\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(\"games_feat shape after merges:\", games_feat.shape)\n",
    "display(games_feat.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2494486-5c6f-4391-b394-56ea23586c80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "games_with_scripts = games_feat.merge(\n",
    "    scripts_df[[\"GAME_ID\", \"SCRIPT_LABEL\", \"SCRIPT_INDEX\"]],\n",
    "    on=\"GAME_ID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(\"games_with_scripts shape:\", games_with_scripts.shape)\n",
    "display(games_with_scripts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d89d4-2500-4ba5-af52-854f1d2006ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_cols = [\n",
    "    \"HOME_OFF_RATING\",\n",
    "    \"HOME_DEF_RATING\",\n",
    "    \"HOME_NET_RATING\",\n",
    "    \"HOME_PACE\",\n",
    "    \"HOME_PIE\",\n",
    "    \"AWAY_OFF_RATING\",\n",
    "    \"AWAY_DEF_RATING\",\n",
    "    \"AWAY_NET_RATING\",\n",
    "    \"AWAY_PACE\",\n",
    "    \"AWAY_PIE\",\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in feature_cols if c in games_with_scripts.columns]\n",
    "\n",
    "print(\"Using feature columns:\", feature_cols)\n",
    "\n",
    "# Drop rows where we don't have a script label (shouldn't be many, but just in case)\n",
    "model_df = games_with_scripts.dropna(subset=[\"SCRIPT_INDEX\"]).copy()\n",
    "\n",
    "X = model_df[feature_cols].values.astype(float)\n",
    "y = model_df[\"SCRIPT_INDEX\"].values.astype(int)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Script label counts:\", np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd55b2-1a89-4f44-9bc8-abb0170c3a0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Track original row indices so we can map predictions back to model_df rows\n",
    "idx = np.arange(len(model_df))\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    idx,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification report (SCRIPT_INDEX):\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a0c21-64c3-4935-b9a9-69a294a5b6f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(X_test)\n",
    "\n",
    "print(\"probs shape:\", probs.shape)  # (n_test, 3)\n",
    "\n",
    "# Map class indices (0=GRIND, 1=BALANCED, 2=CHAOS) to columns in probs\n",
    "class_order = list(clf.classes_)   # e.g. [0, 1, 2]\n",
    "i_grind = class_order.index(0)\n",
    "i_bal   = class_order.index(1)\n",
    "i_chaos = class_order.index(2)\n",
    "\n",
    "# Take the first 10 test samples\n",
    "test_idx_subset = idx_test[:10]\n",
    "\n",
    "# Columns weâ€™d like to see for context\n",
    "preview_cols = [\n",
    "    \"GAME_ID\",\n",
    "    \"HOME_TEAM_NAME\",\n",
    "    \"AWAY_TEAM_NAME\",\n",
    "    \"TOTAL_POINTS\",\n",
    "    \"SCRIPT_LABEL\",\n",
    "]\n",
    "\n",
    "# Keep only the ones that actually exist in model_df\n",
    "preview_cols = [c for c in preview_cols if c in model_df.columns]\n",
    "\n",
    "# Grab those rows from model_df\n",
    "results_preview = model_df.iloc[test_idx_subset][preview_cols].copy()\n",
    "\n",
    "# Attach predicted probabilities\n",
    "results_preview[\"P_GRIND\"]    = probs[:10, i_grind]\n",
    "results_preview[\"P_BALANCED\"] = probs[:10, i_bal]\n",
    "results_preview[\"P_CHAOS\"]    = probs[:10, i_chaos]\n",
    "\n",
    "display(results_preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f4b37-e284-4268-85c6-d438210da931",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "out_dir = PROJECT_ROOT / \"data\" / \"processed\" / \"nba\" / \"models\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Sanity-check that clf and model_df exist\n",
    "print(\"clf type:\", type(clf))\n",
    "print(\"model_df shape:\", model_df.shape)\n",
    "\n",
    "# 2) Save the classifier\n",
    "model_path = out_dir / f\"script_classifier_rf_{season}.joblib\"\n",
    "joblib.dump(clf, model_path)\n",
    "print(\"Saved model to:\", model_path)\n",
    "\n",
    "# 3) Copy model_df and drop duplicate columns to keep parquet happy\n",
    "model_df_out = model_df.copy()\n",
    "\n",
    "# If there are duplicate column names (common after merges), parquet will explode.\n",
    "# This keeps only the first occurrence of each column name.\n",
    "model_df_out = model_df_out.loc[:, ~model_df_out.columns.duplicated()]\n",
    "\n",
    "print(\"model_df_out shape after dropping duplicate columns:\", model_df_out.shape)\n",
    "\n",
    "dataset_path = out_dir / f\"script_classifier_dataset_{season}.parquet\"\n",
    "model_df_out.to_parquet(dataset_path, index=False)\n",
    "print(\"Saved dataset to:\", dataset_path)\n",
    "\n",
    "print(\"âœ… Saved model + dataset to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30bfd2-aa4c-49a2-85d9-bf7f22d6c41c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
