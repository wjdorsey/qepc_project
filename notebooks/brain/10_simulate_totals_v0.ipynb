{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20611746-b355-4c61-9042-63aa72e77f70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[1]\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"qepc in root?\", (PROJECT_ROOT / \"qepc\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5414d-3ad6-48d4-8e33-73e466fbf7be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from qepc.brain.games_loader import fetch_league_games, build_games_table\n",
    "from qepc.brain.scripts import label_game_scripts_by_total_points\n",
    "from qepc.brain.lambda_builder import build_script_level_lambdas\n",
    "\n",
    "season = \"2023-24\"\n",
    "\n",
    "team_games = fetch_league_games(season)\n",
    "games_df = build_games_table(team_games)\n",
    "\n",
    "print(\"games_df rows:\", len(games_df))\n",
    "display(games_df.head())\n",
    "\n",
    "scripts_df = label_game_scripts_by_total_points(\n",
    "    games_df,\n",
    "    low_quantile=0.25,\n",
    "    high_quantile=0.75,\n",
    ")\n",
    "\n",
    "print(\"scripts_df rows:\", len(scripts_df))\n",
    "display(scripts_df.head())\n",
    "\n",
    "script_lambdas = build_script_level_lambdas(games_df, scripts_df)\n",
    "\n",
    "print(\"Script-level lambdas:\")\n",
    "display(script_lambdas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c098312-584f-4579-9212-30a9a22f716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from qepc.brain.teams_loader import fetch_league_team_season_stats\n",
    "import numpy as np\n",
    "\n",
    "# Advanced team stats\n",
    "team_stats_adv = fetch_league_team_season_stats(\n",
    "    season,\n",
    "    measure_type=\"Advanced\",\n",
    ")\n",
    "\n",
    "adv_cols_keep = [\n",
    "    \"TEAM_ID\",\n",
    "    \"TEAM_NAME\",\n",
    "    \"TEAM_ABBREVIATION\",\n",
    "    \"GP\",\n",
    "    \"W\",\n",
    "    \"L\",\n",
    "    \"W_PCT\",\n",
    "    \"MIN\",\n",
    "    \"OFF_RATING\",\n",
    "    \"DEF_RATING\",\n",
    "    \"NET_RATING\",\n",
    "    \"PACE\",\n",
    "    \"PIE\",\n",
    "]\n",
    "adv_cols_keep = [c for c in adv_cols_keep if c in team_stats_adv.columns]\n",
    "team_adv_small = team_stats_adv[adv_cols_keep].copy()\n",
    "\n",
    "home_adv = team_adv_small.add_prefix(\"HOME_\")\n",
    "away_adv = team_adv_small.add_prefix(\"AWAY_\")\n",
    "\n",
    "games_feat = games_df.copy()\n",
    "\n",
    "games_feat = games_feat.merge(\n",
    "    home_adv,\n",
    "    left_on=\"HOME_TEAM_ID\",\n",
    "    right_on=\"HOME_TEAM_ID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "games_feat = games_feat.merge(\n",
    "    away_adv,\n",
    "    left_on=\"AWAY_TEAM_ID\",\n",
    "    right_on=\"AWAY_TEAM_ID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "games_with_scripts = games_feat.merge(\n",
    "    scripts_df[[\"GAME_ID\", \"SCRIPT_LABEL\", \"SCRIPT_INDEX\"]],\n",
    "    on=\"GAME_ID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(\"games_with_scripts shape:\", games_with_scripts.shape)\n",
    "display(games_with_scripts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c47095-9164-43e5-8ceb-d3743a513046",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_dir = PROJECT_ROOT / \"data\" / \"processed\" / \"nba\" / \"models\"\n",
    "model_path = model_dir / f\"script_classifier_rf_{season}.joblib\"\n",
    "\n",
    "print(\"Loading classifier from:\", model_path)\n",
    "clf = joblib.load(model_path)\n",
    "print(\"Loaded classifier. Classes:\", clf.classes_)\n",
    "\n",
    "feature_cols = [\n",
    "    \"HOME_OFF_RATING\",\n",
    "    \"HOME_DEF_RATING\",\n",
    "    \"HOME_NET_RATING\",\n",
    "    \"HOME_PACE\",\n",
    "    \"HOME_PIE\",\n",
    "    \"AWAY_OFF_RATING\",\n",
    "    \"AWAY_DEF_RATING\",\n",
    "    \"AWAY_NET_RATING\",\n",
    "    \"AWAY_PACE\",\n",
    "    \"AWAY_PIE\",\n",
    "]\n",
    "feature_cols = [c for c in feature_cols if c in games_with_scripts.columns]\n",
    "print(\"Using feature columns:\", feature_cols)\n",
    "\n",
    "model_df = games_with_scripts.dropna(subset=[\"SCRIPT_INDEX\"]).copy()\n",
    "X_all = model_df[feature_cols].values.astype(float)\n",
    "\n",
    "print(\"model_df shape:\", model_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a3be8-a1e2-4388-b46f-111ec02066c5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from qepc.brain.lambda_builder import build_team_script_lambdas\n",
    "\n",
    "team_script_lambdas = build_team_script_lambdas(\n",
    "    team_games_df=team_games,\n",
    "    games_df=games_df,\n",
    "    scripts_df=scripts_df,\n",
    ")\n",
    "\n",
    "print(\"team_script_lambdas shape:\", team_script_lambdas.shape)\n",
    "display(team_script_lambdas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf789d-795b-45d4-8e15-1281ea1c1004",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_team_script_param_dict(team_script_lambdas_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build a dict mapping (TEAM_ID, TEAM_ROLE, SCRIPT_LABEL) -> (mean_team_pts, std_team_pts)\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for _, row in team_script_lambdas_df.iterrows():\n",
    "        key = (int(row[\"TEAM_ID\"]), row[\"TEAM_ROLE\"], row[\"SCRIPT_LABEL\"])\n",
    "        mean_t = float(row[\"mean_team_pts\"])\n",
    "        std_t = float(row[\"std_team_pts\"]) if not np.isnan(row[\"std_team_pts\"]) else 0.0\n",
    "        d[key] = (mean_t, std_t)\n",
    "    return d\n",
    "\n",
    "\n",
    "team_script_params = build_team_script_param_dict(team_script_lambdas)\n",
    "print(\"Sample entries from team_script_params (first 5):\")\n",
    "for i, (k, v) in enumerate(team_script_params.items()):\n",
    "    print(k, \"->\", v)\n",
    "    if i >= 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f4ed6-e77f-4c6b-a2ea-b9fd15a3944e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fallback: global script-level means for team points (approx half total)\n",
    "script_fallback = {}\n",
    "for _, row in script_lambdas.iterrows():\n",
    "    label = row[\"SCRIPT_LABEL\"]\n",
    "    mean_total = float(row[\"mean_total_pts\"])\n",
    "    std_total = float(row[\"std_total_pts\"]) if not np.isnan(row[\"std_total_pts\"]) else 0.0\n",
    "    script_fallback[label] = (mean_total / 2.0, std_total / 2.0)\n",
    "\n",
    "print(\"Script-level fallback (approx per-team Î»):\", script_fallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d92abf-ab2f-47cc-a33e-3464bd2c6e52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def simulate_team_totals_for_row(\n",
    "    game_row: pd.Series,\n",
    "    x_row: np.ndarray,\n",
    "    clf,\n",
    "    team_script_params: dict,\n",
    "    script_fallback: dict,\n",
    "    n_sims: int = 5000,\n",
    "    random_state: int | None = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate home/away/team totals for a single game using:\n",
    "      - script probabilities from clf\n",
    "      - team+script mean/std from team_script_params\n",
    "      - fallback per-script means if team+script is missing\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Extract home/away team IDs\n",
    "    if \"HOME_TEAM_ID\" not in game_row.index or \"AWAY_TEAM_ID\" not in game_row.index:\n",
    "        raise ValueError(\"game_row must contain HOME_TEAM_ID and AWAY_TEAM_ID\")\n",
    "\n",
    "    home_id = int(game_row[\"HOME_TEAM_ID\"])\n",
    "    away_id = int(game_row[\"AWAY_TEAM_ID\"])\n",
    "\n",
    "    # --- 1) Script probabilities from classifier ---\n",
    "    probs = clf.predict_proba(x_row)[0]\n",
    "    class_order = list(clf.classes_)  # [0,1,2] mapping to indices\n",
    "\n",
    "    # Map class index -> script label\n",
    "    idx_to_label = {0: \"GRIND\", 1: \"BALANCED\", 2: \"CHAOS\"}\n",
    "\n",
    "    p_grind = float(probs[class_order.index(0)])\n",
    "    p_bal   = float(probs[class_order.index(1)])\n",
    "    p_chaos = float(probs[class_order.index(2)])\n",
    "\n",
    "    p_vec = np.array([p_grind, p_bal, p_chaos])\n",
    "    labels = np.array([\"GRIND\", \"BALANCED\", \"CHAOS\"])\n",
    "\n",
    "    # --- 2) Expected totals (analytic, using team-level params) ---\n",
    "\n",
    "    expected_home = 0.0\n",
    "    expected_away = 0.0\n",
    "\n",
    "    for label, p in zip(labels, p_vec):\n",
    "        # Home\n",
    "        key_home = (home_id, \"HOME\", label)\n",
    "        if key_home in team_script_params:\n",
    "            mean_home, _ = team_script_params[key_home]\n",
    "        else:\n",
    "            mean_home, _ = script_fallback[label]\n",
    "\n",
    "        # Away\n",
    "        key_away = (away_id, \"AWAY\", label)\n",
    "        if key_away in team_script_params:\n",
    "            mean_away, _ = team_script_params[key_away]\n",
    "        else:\n",
    "            mean_away, _ = script_fallback[label]\n",
    "\n",
    "        expected_home += p * mean_home\n",
    "        expected_away += p * mean_away\n",
    "\n",
    "    expected_total = expected_home + expected_away\n",
    "\n",
    "    # --- 3) Monte Carlo simulation ---\n",
    "\n",
    "    # Sample scripts for each universe\n",
    "    script_indices = rng.choice(len(labels), size=n_sims, p=p_vec)\n",
    "    sampled_labels = labels[script_indices]\n",
    "\n",
    "    samples_home = np.empty(n_sims, dtype=float)\n",
    "    samples_away = np.empty(n_sims, dtype=float)\n",
    "\n",
    "    for i, label in enumerate(sampled_labels):\n",
    "        # Home team points\n",
    "        key_home = (home_id, \"HOME\", label)\n",
    "        if key_home in team_script_params:\n",
    "            mean_home, std_home = team_script_params[key_home]\n",
    "        else:\n",
    "            mean_home, std_home = script_fallback[label]\n",
    "\n",
    "        # Away team points\n",
    "        key_away = (away_id, \"AWAY\", label)\n",
    "        if key_away in team_script_params:\n",
    "            mean_away, std_away = team_script_params[key_away]\n",
    "        else:\n",
    "            mean_away, std_away = script_fallback[label]\n",
    "\n",
    "        if std_home <= 0:\n",
    "            samples_home[i] = mean_home\n",
    "        else:\n",
    "            samples_home[i] = rng.normal(loc=mean_home, scale=std_home)\n",
    "\n",
    "        if std_away <= 0:\n",
    "            samples_away[i] = mean_away\n",
    "        else:\n",
    "            samples_away[i] = rng.normal(loc=mean_away, scale=std_away)\n",
    "\n",
    "    # Clip to keep scores sane\n",
    "    samples_home = np.clip(samples_home, 70, 150)\n",
    "    samples_away = np.clip(samples_away, 70, 150)\n",
    "\n",
    "    samples_total = samples_home + samples_away\n",
    "\n",
    "    summary_total = {\n",
    "        \"mean_total\": float(samples_total.mean()),\n",
    "        \"std_total\": float(samples_total.std()),\n",
    "        \"p_over_230\": float((samples_total > 230).mean()),\n",
    "        \"p_over_240\": float((samples_total > 240).mean()),\n",
    "        \"p_under_220\": float((samples_total < 220).mean()),\n",
    "        \"p_between_220_240\": float(\n",
    "            ((samples_total >= 220) & (samples_total <= 240)).mean()\n",
    "        ),\n",
    "        \"p95_total\": float(np.percentile(samples_total, 95)),\n",
    "        \"p05_total\": float(np.percentile(samples_total, 5)),\n",
    "        }\n",
    "\n",
    "    summary_home = {\n",
    "        \"mean_home\": float(samples_home.mean()),\n",
    "        \"std_home\": float(samples_home.std()),\n",
    "        \"p_home_over_110\": float((samples_home > 110).mean()),\n",
    "        \"p05_home\": float(np.percentile(samples_home, 5)),\n",
    "        \"p95_home\": float(np.percentile(samples_home, 95)),\n",
    "        }\n",
    "\n",
    "    summary_away = {\n",
    "        \"mean_away\": float(samples_away.mean()),\n",
    "        \"std_away\": float(samples_away.std()),\n",
    "        \"p_away_over_110\": float((samples_away > 110).mean()),\n",
    "        \"p05_away\": float(np.percentile(samples_away, 5)),\n",
    "        \"p95_away\": float(np.percentile(samples_away, 95)),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"p_grind\": p_grind,\n",
    "        \"p_balanced\": p_bal,\n",
    "        \"p_chaos\": p_chaos,\n",
    "        \"expected_home\": float(expected_home),\n",
    "        \"expected_away\": float(expected_away),\n",
    "        \"expected_total\": float(expected_total),\n",
    "        \"samples_home\": samples_home,\n",
    "        \"samples_away\": samples_away,\n",
    "        \"samples_total\": samples_total,\n",
    "        \"summary_home\": summary_home,\n",
    "        \"summary_away\": summary_away,\n",
    "        \"summary_total\": summary_total,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465ab1a-783d-46b2-b4a4-e978542823b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_script_param_dict(script_lambdas_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build a dict:\n",
    "        label -> (mean_total_pts, std_total_pts)\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for _, row in script_lambdas_df.iterrows():\n",
    "        label = row[\"SCRIPT_LABEL\"]\n",
    "        mean_t = float(row[\"mean_total_pts\"])\n",
    "        std_t = float(row[\"std_total_pts\"]) if not np.isnan(row[\"std_total_pts\"]) else 0.0\n",
    "        d[label] = (mean_t, std_t)\n",
    "    return d\n",
    "\n",
    "\n",
    "script_params = build_script_param_dict(script_lambdas)\n",
    "print(\"script_params:\", script_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688167c-a56b-4114-a717-9fa668e30aa6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def simulate_game_totals_for_row(\n",
    "    game_row: pd.Series,\n",
    "    x_row: np.ndarray,\n",
    "    clf,\n",
    "    script_params: dict,\n",
    "    n_sims: int = 5000,\n",
    "    random_state: int | None = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate total points for a single game using:\n",
    "      - script probabilities from clf\n",
    "      - per-script mean/std from script_params\n",
    "      - Normal approximation for totals in each script\n",
    "\n",
    "    Returns:\n",
    "      dict with:\n",
    "        - p_grind, p_balanced, p_chaos\n",
    "        - expected_total_qepc\n",
    "        - samples (np.array of simulated totals)\n",
    "        - summary stats (mean, std, percentiles)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # 1) Script probabilities from classifier\n",
    "    probs = clf.predict_proba(x_row)[0]\n",
    "    class_order = list(clf.classes_)  # [0,1,2] mapping to [GRIND,BALANCED,CHAOS]\n",
    "\n",
    "    # Map classes to script labels\n",
    "    label_for_class = {\n",
    "        0: \"GRIND\",\n",
    "        1: \"BALANCED\",\n",
    "        2: \"CHAOS\",\n",
    "    }\n",
    "    # Build a vector of probabilities in GRIND/BALANCED/CHAOS order\n",
    "    p_grind = float(probs[class_order.index(0)])\n",
    "    p_bal   = float(probs[class_order.index(1)])\n",
    "    p_chaos = float(probs[class_order.index(2)])\n",
    "\n",
    "    p_vec = np.array([p_grind, p_bal, p_chaos])\n",
    "    labels = np.array([\"GRIND\", \"BALANCED\", \"CHAOS\"])\n",
    "\n",
    "    # 2) Expected total from mixture (sanity check)\n",
    "    expected_total_qepc = 0.0\n",
    "    for label, p in zip(labels, p_vec):\n",
    "        mean_t, _ = script_params[label]\n",
    "        expected_total_qepc += p * mean_t\n",
    "\n",
    "    # 3) Simulate scripts\n",
    "    script_indices = rng.choice(len(labels), size=n_sims, p=p_vec)\n",
    "    sampled_labels = labels[script_indices]\n",
    "\n",
    "    # 4) For each simulation, draw total from Normal(mean_s, std_s)\n",
    "    samples = np.empty(n_sims, dtype=float)\n",
    "    for i, label in enumerate(sampled_labels):\n",
    "        mean_t, std_t = script_params[label]\n",
    "        if std_t <= 0:\n",
    "            samples[i] = mean_t\n",
    "        else:\n",
    "            samples[i] = rng.normal(loc=mean_t, scale=std_t)\n",
    "\n",
    "    # (Optional) we can clip unrealistic totals\n",
    "    samples = np.clip(samples, 120, 320)  # arbitrary, but keeps extremes sane\n",
    "\n",
    "    # 5) Summaries\n",
    "    summary = {\n",
    "        \"mean\": float(samples.mean()),\n",
    "        \"std\": float(samples.std()),\n",
    "        \"p_over_230\": float((samples > 230).mean()),\n",
    "        \"p_over_240\": float((samples > 240).mean()),\n",
    "        \"p_under_220\": float((samples < 220).mean()),\n",
    "        \"p_between_220_240\": float(((samples >= 220) & (samples <= 240)).mean()),\n",
    "        \"p95\": float(np.percentile(samples, 95)),\n",
    "        \"p05\": float(np.percentile(samples, 5)),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"p_grind\": p_grind,\n",
    "        \"p_balanced\": p_bal,\n",
    "        \"p_chaos\": p_chaos,\n",
    "        \"expected_total_qepc\": float(expected_total_qepc),\n",
    "        \"samples\": samples,\n",
    "        \"summary\": summary,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984aab40-fb1a-4fa5-92b8-2e154751d555",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Choose a game\n",
    "row_idx = 0  # you can change this\n",
    "\n",
    "game_row = model_df.iloc[row_idx]\n",
    "x_row = X_all[row_idx : row_idx + 1]\n",
    "\n",
    "print(\"Game info (available columns):\")\n",
    "print(list(game_row.index))\n",
    "\n",
    "candidate_cols = [\n",
    "    \"GAME_ID\",\n",
    "    \"GAME_DATE\",\n",
    "    \"HOME_TEAM_NAME\",\n",
    "    \"AWAY_TEAM_NAME\",\n",
    "    \"HOME_TEAM\",\n",
    "    \"AWAY_TEAM\",\n",
    "    \"HOME_TEAM_ABBREVIATION\",\n",
    "    \"AWAY_TEAM_ABBREVIATION\",\n",
    "    \"TOTAL_POINTS\",\n",
    "    \"SCRIPT_LABEL\",\n",
    "]\n",
    "preview_cols = [c for c in candidate_cols if c in game_row.index]\n",
    "\n",
    "print(\"\\nDemo game:\")\n",
    "display(game_row[preview_cols])\n",
    "\n",
    "result = simulate_game_totals_for_row(\n",
    "    game_row=game_row,\n",
    "    x_row=x_row,\n",
    "    clf=clf,\n",
    "    script_params=script_params,\n",
    "    n_sims=5000,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "print(\"\\nScript probabilities:\")\n",
    "print(f\"P_GRIND    = {result['p_grind']:.3f}\")\n",
    "print(f\"P_BALANCED = {result['p_balanced']:.3f}\")\n",
    "print(f\"P_CHAOS    = {result['p_chaos']:.3f}\")\n",
    "print(f\"Sum        = {result['p_grind'] + result['p_balanced'] + result['p_chaos']:.3f}\")\n",
    "\n",
    "print(\"\\nExpected total from mixture:\", f\"{result['expected_total_qepc']:.1f}\")\n",
    "\n",
    "print(\"\\nSimulated summary from multiverse:\")\n",
    "for k, v in result[\"summary\"].items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n",
    "\n",
    "actual_total = float(game_row[\"TOTAL_POINTS\"])\n",
    "print(f\"\\nActual final total: {actual_total:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489b860-befb-4369-9936-26d55e5ab368",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_results = []\n",
    "\n",
    "n_sims_per_game = 2000  # you can tweak this up/down\n",
    "\n",
    "n_games = len(model_df)\n",
    "print(f\"Simulating {n_games} games with {n_sims_per_game} sims each...\")\n",
    "\n",
    "for i in range(n_games):\n",
    "    game_row = model_df.iloc[i]\n",
    "    x_row = X_all[i : i + 1]  # keep 2D for sklearn\n",
    "\n",
    "    res = simulate_game_totals_for_row(\n",
    "        game_row=game_row,\n",
    "        x_row=x_row,\n",
    "        clf=clf,\n",
    "        script_params=script_params,\n",
    "        n_sims=n_sims_per_game,\n",
    "        random_state=123 + i,  # different seed per game\n",
    "    )\n",
    "\n",
    "    actual_total = float(game_row[\"TOTAL_POINTS\"])\n",
    "\n",
    "    # Grab script label if present\n",
    "    script_label = game_row.get(\"SCRIPT_LABEL\", None)\n",
    "\n",
    "    summary = res[\"summary\"]\n",
    "\n",
    "    all_results.append(\n",
    "        {\n",
    "            \"GAME_ID\": game_row[\"GAME_ID\"],\n",
    "            \"SCRIPT_LABEL\": script_label,\n",
    "            \"ACTUAL_TOTAL\": actual_total,\n",
    "            \"P_GRIND\": res[\"p_grind\"],\n",
    "            \"P_BALANCED\": res[\"p_balanced\"],\n",
    "            \"P_CHAOS\": res[\"p_chaos\"],\n",
    "            \"EXPECTED_TOTAL_QEPC\": res[\"expected_total_qepc\"],\n",
    "            \"MEAN_SIM\": summary[\"mean\"],\n",
    "            \"STD_SIM\": summary[\"std\"],\n",
    "            \"P_OVER_230\": summary[\"p_over_230\"],\n",
    "            \"P_OVER_240\": summary[\"p_over_240\"],\n",
    "            \"P_UNDER_220\": summary[\"p_under_220\"],\n",
    "            \"P_BETWEEN_220_240\": summary[\"p_between_220_240\"],\n",
    "            \"P05\": summary[\"p05\"],\n",
    "            \"P95\": summary[\"p95\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  ... {i + 1}/{n_games} games simulated\")\n",
    "\n",
    "eval_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\nEvaluation dataframe shape:\", eval_df.shape)\n",
    "display(eval_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03270a91-ada0-4fdb-9927-19338c222c32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Is the actual total inside the simulated 90% interval [P05, P95]?\n",
    "eval_df[\"INSIDE_90\"] = (\n",
    "    (eval_df[\"ACTUAL_TOTAL\"] >= eval_df[\"P05\"])\n",
    "    & (eval_df[\"ACTUAL_TOTAL\"] <= eval_df[\"P95\"])\n",
    ")\n",
    "\n",
    "coverage_90 = eval_df[\"INSIDE_90\"].mean()\n",
    "\n",
    "print(f\"Fraction of games where actual total is inside [P05, P95]: {coverage_90:.3f}\")\n",
    "print(f\"Expected for a well-calibrated 90% interval: ~0.90\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9169c7d-d8f1-4c67-b55a-6d068329ffd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if \"SCRIPT_LABEL\" in eval_df.columns:\n",
    "    coverage_by_script = eval_df.groupby(\"SCRIPT_LABEL\")[\"INSIDE_90\"].mean()\n",
    "    print(\"Coverage by script label:\")\n",
    "    display(coverage_by_script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e56683-3d10-45b3-8442-400b35d52b99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "actual = eval_df[\"ACTUAL_TOTAL\"].values\n",
    "pred_sim_mean = eval_df[\"MEAN_SIM\"].values\n",
    "\n",
    "mae_sim = mean_absolute_error(actual, pred_sim_mean)\n",
    "rmse_sim = np.sqrt(mean_squared_error(actual, pred_sim_mean))\n",
    "\n",
    "print(f\"Simulation-based mean forecast:\")\n",
    "print(f\"  MAE  : {mae_sim:.3f}\")\n",
    "print(f\"  RMSE : {rmse_sim:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2673f5-f72e-4d7b-91ef-ec3b4568e652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Choose a game (same as before for comparison)\n",
    "row_idx = 0  # you can change this\n",
    "\n",
    "game_row = model_df.iloc[row_idx]\n",
    "x_row = X_all[row_idx : row_idx + 1]\n",
    "\n",
    "candidate_cols = [\n",
    "    \"GAME_ID\",\n",
    "    \"GAME_DATE\",\n",
    "    \"HOME_TEAM_ABBREVIATION\",\n",
    "    \"AWAY_TEAM_ABBREVIATION\",\n",
    "    \"HOME_TEAM_NAME\",\n",
    "    \"AWAY_TEAM_NAME\",\n",
    "    \"TOTAL_POINTS\",\n",
    "    \"SCRIPT_LABEL\",\n",
    "]\n",
    "preview_cols = [c for c in candidate_cols if c in game_row.index]\n",
    "\n",
    "print(\"Game info:\")\n",
    "display(game_row[preview_cols])\n",
    "\n",
    "result_team = simulate_team_totals_for_row(\n",
    "    game_row=game_row,\n",
    "    x_row=x_row,\n",
    "    clf=clf,\n",
    "    team_script_params=team_script_params,\n",
    "    script_fallback=script_fallback,\n",
    "    n_sims=5000,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "print(\"\\nScript probabilities (from classifier):\")\n",
    "print(f\"P_GRIND    = {result_team['p_grind']:.3f}\")\n",
    "print(f\"P_BALANCED = {result_team['p_balanced']:.3f}\")\n",
    "print(f\"P_CHAOS    = {result_team['p_chaos']:.3f}\")\n",
    "\n",
    "print(\"\\nAnalytic expectations (team-aware):\")\n",
    "print(f\"Expected home pts:  {result_team['expected_home']:.1f}\")\n",
    "print(f\"Expected away pts:  {result_team['expected_away']:.1f}\")\n",
    "print(f\"Expected total pts: {result_team['expected_total']:.1f}\")\n",
    "\n",
    "actual_total = float(game_row[\"TOTAL_POINTS\"])\n",
    "print(f\"\\nActual final total: {actual_total:.1f}\")\n",
    "\n",
    "print(\"\\nSimulation summary (home):\")\n",
    "for k, v in result_team[\"summary_home\"].items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n",
    "\n",
    "print(\"\\nSimulation summary (away):\")\n",
    "for k, v in result_team[\"summary_away\"].items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n",
    "\n",
    "print(\"\\nSimulation summary (total):\")\n",
    "for k, v in result_team[\"summary_total\"].items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1861da0-9be1-4aef-bab5-6fb570e6aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# team_games came from fetch_league_games(season)\n",
    "# games_df came from build_games_table(team_games)\n",
    "\n",
    "if \"PTS\" in team_games.columns:\n",
    "    pts_col = \"PTS\"\n",
    "elif \"TEAM_POINTS\" in team_games.columns:\n",
    "    pts_col = \"TEAM_POINTS\"\n",
    "else:\n",
    "    raise ValueError(\"team_games must contain 'PTS' or 'TEAM_POINTS'.\")\n",
    "\n",
    "team_pts = team_games[[\"GAME_ID\", \"TEAM_ID\", pts_col]].copy()\n",
    "team_pts = team_pts.rename(columns={pts_col: \"TEAM_POINTS\"})\n",
    "\n",
    "gsmall = games_df[[\"GAME_ID\", \"HOME_TEAM_ID\", \"AWAY_TEAM_ID\"]].copy()\n",
    "\n",
    "merged_scores = team_pts.merge(gsmall, on=\"GAME_ID\", how=\"left\")\n",
    "\n",
    "home_scores = merged_scores[\n",
    "    merged_scores[\"TEAM_ID\"] == merged_scores[\"HOME_TEAM_ID\"]\n",
    "][[\"GAME_ID\", \"TEAM_POINTS\"]].rename(columns={\"TEAM_POINTS\": \"HOME_PTS\"})\n",
    "\n",
    "away_scores = merged_scores[\n",
    "    merged_scores[\"TEAM_ID\"] == merged_scores[\"AWAY_TEAM_ID\"]\n",
    "][[\"GAME_ID\", \"TEAM_POINTS\"]].rename(columns={\"TEAM_POINTS\": \"AWAY_PTS\"})\n",
    "\n",
    "game_scores = home_scores.merge(away_scores, on=\"GAME_ID\", how=\"inner\")\n",
    "\n",
    "game_scores[\"GAME_ID\"] = game_scores[\"GAME_ID\"].astype(str)\n",
    "\n",
    "print(\"game_scores shape:\", game_scores.shape)\n",
    "display(game_scores.head())\n",
    "\n",
    "score_map = game_scores.set_index(\"GAME_ID\")[[\"HOME_PTS\", \"AWAY_PTS\"]].to_dict(\"index\")\n",
    "print(\"Sample score_map entry:\", next(iter(score_map.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108615f-0638-4637-8388-be97f5ee61ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_team_results = []\n",
    "\n",
    "n_sims_per_game = 2000\n",
    "n_games = len(model_df)\n",
    "print(f\"Simulating {n_games} games with {n_sims_per_game} sims each (team-aware)...\")\n",
    "\n",
    "for i in range(n_games):\n",
    "    game_row = model_df.iloc[i]\n",
    "    x_row = X_all[i : i + 1]\n",
    "\n",
    "    game_id_str = str(game_row[\"GAME_ID\"])\n",
    "    if game_id_str not in score_map:\n",
    "        continue\n",
    "\n",
    "    actual_home = float(score_map[game_id_str][\"HOME_PTS\"])\n",
    "    actual_away = float(score_map[game_id_str][\"AWAY_PTS\"])\n",
    "    actual_total = actual_home + actual_away\n",
    "\n",
    "    res = simulate_team_totals_for_row(\n",
    "        game_row=game_row,\n",
    "        x_row=x_row,\n",
    "        clf=clf,\n",
    "        team_script_params=team_script_params,\n",
    "        script_fallback=script_fallback,\n",
    "        n_sims=n_sims_per_game,\n",
    "        random_state=123 + i,\n",
    "    )\n",
    "\n",
    "    sh = res[\"summary_home\"]\n",
    "    sa = res[\"summary_away\"]\n",
    "    st = res[\"summary_total\"]\n",
    "\n",
    "    all_team_results.append(\n",
    "        {\n",
    "            \"GAME_ID\": game_id_str,\n",
    "            \"SCRIPT_LABEL\": game_row.get(\"SCRIPT_LABEL\", None),\n",
    "            \"ACTUAL_HOME\": actual_home,\n",
    "            \"ACTUAL_AWAY\": actual_away,\n",
    "            \"ACTUAL_TOTAL\": actual_total,\n",
    "            \"EXP_HOME\": res[\"expected_home\"],\n",
    "            \"EXP_AWAY\": res[\"expected_away\"],\n",
    "            \"EXP_TOTAL\": res[\"expected_total\"],\n",
    "            \"MEAN_HOME_SIM\": sh[\"mean_home\"],\n",
    "            \"MEAN_AWAY_SIM\": sa[\"mean_away\"],\n",
    "            \"MEAN_TOTAL_SIM\": st[\"mean_total\"],\n",
    "            \"HOME_P05\": sh[\"p05_home\"],\n",
    "            \"HOME_P95\": sh[\"p95_home\"],\n",
    "            \"AWAY_P05\": sa[\"p05_away\"],\n",
    "            \"AWAY_P95\": sa[\"p95_away\"],\n",
    "            \"TOTAL_P05\": st[\"p05_total\"],\n",
    "            \"TOTAL_P95\": st[\"p95_total\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  ... {i + 1}/{n_games} games processed\")\n",
    "\n",
    "eval_team_df = pd.DataFrame(all_team_results)\n",
    "\n",
    "print(\"\\nEvaluation team-level dataframe shape:\", eval_team_df.shape)\n",
    "display(eval_team_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645fa7e-d576-40c0-8a49-8394a8c60d35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "league_home_avg = game_scores[\"HOME_PTS\"].mean()\n",
    "league_away_avg = game_scores[\"AWAY_PTS\"].mean()\n",
    "league_total_avg = games_df[\"TOTAL_POINTS\"].mean()\n",
    "\n",
    "print(f\"League average home pts : {league_home_avg:.2f}\")\n",
    "print(f\"League average away pts : {league_away_avg:.2f}\")\n",
    "print(f\"League average total pts: {league_total_avg:.2f}\")\n",
    "\n",
    "act_home = eval_team_df[\"ACTUAL_HOME\"].values\n",
    "act_away = eval_team_df[\"ACTUAL_AWAY\"].values\n",
    "act_total = eval_team_df[\"ACTUAL_TOTAL\"].values\n",
    "\n",
    "pred_home_qepc = eval_team_df[\"EXP_HOME\"].values\n",
    "pred_away_qepc = eval_team_df[\"EXP_AWAY\"].values\n",
    "pred_total_qepc = eval_team_df[\"EXP_TOTAL\"].values\n",
    "\n",
    "pred_home_base = np.full_like(act_home, league_home_avg, dtype=float)\n",
    "pred_away_base = np.full_like(act_away, league_away_avg, dtype=float)\n",
    "pred_total_base = np.full_like(act_total, league_total_avg, dtype=float)\n",
    "\n",
    "def mae_rmse(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return mae, rmse\n",
    "\n",
    "mae_home_q, rmse_home_q = mae_rmse(act_home, pred_home_qepc)\n",
    "mae_home_b, rmse_home_b = mae_rmse(act_home, pred_home_base)\n",
    "\n",
    "mae_away_q, rmse_away_q = mae_rmse(act_away, pred_away_qepc)\n",
    "mae_away_b, rmse_away_b = mae_rmse(act_away, pred_away_base)\n",
    "\n",
    "mae_total_q, rmse_total_q = mae_rmse(act_total, pred_total_qepc)\n",
    "mae_total_b, rmse_total_b = mae_rmse(act_total, pred_total_base)\n",
    "\n",
    "print(\"\\nHome points forecast:\")\n",
    "print(f\"  QEPC   MAE: {mae_home_q:.3f}, RMSE: {rmse_home_q:.3f}\")\n",
    "print(f\"  Base   MAE: {mae_home_b:.3f}, RMSE: {rmse_home_b:.3f}\")\n",
    "\n",
    "print(\"\\nAway points forecast:\")\n",
    "print(f\"  QEPC   MAE: {mae_away_q:.3f}, RMSE: {rmse_away_q:.3f}\")\n",
    "print(f\"  Base   MAE: {mae_away_b:.3f}, RMSE: {rmse_away_b:.3f}\")\n",
    "\n",
    "print(\"\\nTotal points forecast (team-aware):\")\n",
    "print(f\"  QEPC   MAE: {mae_total_q:.3f}, RMSE: {rmse_total_q:.3f}\")\n",
    "print(f\"  Base   MAE: {mae_total_b:.3f}, RMSE: {rmse_total_b:.3f}\")\n",
    "\n",
    "print(\"\\nImprovements (baseline - QEPC):\")\n",
    "print(f\"  Home  MAE: {mae_home_b - mae_home_q:.3f}, RMSE: {rmse_home_b - rmse_home_q:.3f}\")\n",
    "print(f\"  Away  MAE: {mae_away_b - mae_away_q:.3f}, RMSE: {rmse_away_b - rmse_away_q:.3f}\")\n",
    "print(f\"  Total MAE: {mae_total_b - mae_total_q:.3f}, RMSE: {rmse_total_b - rmse_total_q:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e50a4-3f86-4128-9c1f-020c7fd37cea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eval_team_df[\"HOME_IN_90\"] = (\n",
    "    (eval_team_df[\"ACTUAL_HOME\"] >= eval_team_df[\"HOME_P05\"])\n",
    "    & (eval_team_df[\"ACTUAL_HOME\"] <= eval_team_df[\"HOME_P95\"])\n",
    ")\n",
    "\n",
    "eval_team_df[\"AWAY_IN_90\"] = (\n",
    "    (eval_team_df[\"ACTUAL_AWAY\"] >= eval_team_df[\"AWAY_P05\"])\n",
    "    & (eval_team_df[\"ACTUAL_AWAY\"] <= eval_team_df[\"AWAY_P95\"])\n",
    ")\n",
    "\n",
    "eval_team_df[\"TOTAL_IN_90\"] = (\n",
    "    (eval_team_df[\"ACTUAL_TOTAL\"] >= eval_team_df[\"TOTAL_P05\"])\n",
    "    & (eval_team_df[\"ACTUAL_TOTAL\"] <= eval_team_df[\"TOTAL_P95\"])\n",
    ")\n",
    "\n",
    "cov_home = eval_team_df[\"HOME_IN_90\"].mean()\n",
    "cov_away = eval_team_df[\"AWAY_IN_90\"].mean()\n",
    "cov_total = eval_team_df[\"TOTAL_IN_90\"].mean()\n",
    "\n",
    "print(f\"Home  coverage [P05,P95]: {cov_home:.3f}\")\n",
    "print(f\"Away  coverage [P05,P95]: {cov_away:.3f}\")\n",
    "print(f\"Total coverage [P05,P95]: {cov_total:.3f}\")\n",
    "print(\"Target for a 90% interval: ~0.90\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323468c-89d3-4989-914f-3faa8720c1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
