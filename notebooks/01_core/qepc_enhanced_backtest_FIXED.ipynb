{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üî¨ QEPC Enhanced Backtest - FIXED\n",
    "\n",
    "This notebook:\n",
    "1. Uses **actual game results** from your data files\n",
    "2. Compares QEPC predictions to real outcomes\n",
    "3. Calculates detailed accuracy metrics\n",
    "4. Generates visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NotebookHeader] QEPC project root: C:\\Users\\wdors\\qepc_project\n",
      "[NotebookHeader] Project root already on sys.path: C:\\Users\\wdors\\qepc_project\n",
      "[QEPC] Autoload complete.\n",
      "[NotebookHeader] qepc_autoload imported successfully.\n",
      "[NotebookHeader] data_dir: C:\\Users\\wdors\\qepc_project\\data\n",
      "[NotebookHeader] raw_dir:  C:\\Users\\wdors\\qepc_project\\data\\raw\n",
      "[NotebookHeader] Notebook environment ready.\n",
      "‚úÖ QEPC environment initialized\n",
      "project_root: C:\\Users\\wdors\\qepc_project\n",
      "data_dir: C:\\Users\\wdors\\qepc_project\\data\n",
      "raw_dir: C:\\Users\\wdors\\qepc_project\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "# --- Robust bootstrap to load notebook_header.py no matter where Jupyter started ---\n",
    "\n",
    "import sys\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Find the project root: the folder that contains notebook_header.py\n",
    "cur = Path.cwd()\n",
    "project_root = None\n",
    "\n",
    "for _ in range(6):  # walk up a few levels just in case\n",
    "    if (cur / \"notebook_header.py\").exists():\n",
    "        project_root = cur\n",
    "        break\n",
    "    cur = cur.parent\n",
    "\n",
    "if project_root is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find notebook_header.py in the current directory or its parents.\"\n",
    "    )\n",
    "\n",
    "# 2) Make sure project root is on sys.path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# 3) Load notebook_header.py as a proper module\n",
    "header_path = project_root / \"notebook_header.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"notebook_header\", header_path)\n",
    "notebook_header = importlib.util.module_from_spec(spec)\n",
    "\n",
    "# IMPORTANT: register it in sys.modules so @dataclass doesn't break\n",
    "sys.modules[spec.name] = notebook_header\n",
    "\n",
    "spec.loader.exec_module(notebook_header)\n",
    "\n",
    "# 4) Now call qepc_notebook_setup from that module\n",
    "env = notebook_header.qepc_notebook_setup(run_diagnostics=False)\n",
    "data_dir = env.data_dir\n",
    "raw_dir = env.raw_dir\n",
    "\n",
    "from qepc_autoload import qepc_step\n",
    "\n",
    "print(\"‚úÖ QEPC environment initialized\")\n",
    "print(\"project_root:\", project_root)\n",
    "print(\"data_dir:\", data_dir)\n",
    "print(\"raw_dir:\", raw_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Load Actual Game Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir: C:\\Users\\wdors\\qepc_project\\data\n",
      "raw_dir: C:\\Users\\wdors\\qepc_project\\data\\raw\n",
      "\n",
      "üìä Looking for game data...\n",
      "‚úÖ Loaded: TeamStatistics.csv (144,314 rows)\n",
      "\n",
      "üìå Using date column: gameDate\n",
      "‚ö†Ô∏è NaT after generic parse: 143758\n",
      "‚ö†Ô∏è Remaining NaT after m/d/Y H:M parse: 0\n",
      "\n",
      "‚úÖ Remaining rows: 144,314\n",
      "üìÖ Date range: 1946-11-26 to 2025-11-17\n",
      "\n",
      "üìã Columns available: ['gameDate', 'teamCity', 'teamName', 'opponentTeamCity', 'opponentTeamName', 'teamScore', 'opponentScore', 'reboundsTotal', 'assists', 'threePointersMade']...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP: Resolve data_dir and raw_dir using QEPC path helpers\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    # Preferred: use autoload helpers (with get_raw_data_dir if available)\n",
    "    from qepc.autoload.paths import get_data_dir, get_raw_data_dir\n",
    "\n",
    "    data_dir = get_data_dir()\n",
    "    raw_dir = get_raw_data_dir()\n",
    "except ImportError:\n",
    "    # Fallback: only get_data_dir exists\n",
    "    from qepc.autoload.paths import get_data_dir\n",
    "\n",
    "    data_dir = get_data_dir()\n",
    "    raw_dir = data_dir / \"raw\"\n",
    "except Exception:\n",
    "    # Last-resort fallback (shouldn't normally be needed)\n",
    "    print(\"‚ö†Ô∏è Could not import qepc.autoload.paths cleanly, falling back to cwd-based paths.\")\n",
    "    project_root = Path.cwd().parents[0]\n",
    "    data_dir = project_root / \"data\"\n",
    "    raw_dir = data_dir / \"raw\"\n",
    "\n",
    "print(\"data_dir:\", data_dir)\n",
    "print(\"raw_dir:\", raw_dir)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP: Load raw game-level team stats from best available CSV\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nüìä Looking for game data...\")\n",
    "\n",
    "possible_paths = [\n",
    "    raw_dir / \"TeamStatistics.csv\",\n",
    "    data_dir / \"TeamStatistics.csv\",\n",
    "    data_dir / \"GameResults_2025.csv\",\n",
    "    data_dir / \"Games.csv\",\n",
    "]\n",
    "\n",
    "team_stats = None\n",
    "for path in possible_paths:\n",
    "    if path.exists():\n",
    "        try:\n",
    "            team_stats = pd.read_csv(path)\n",
    "            print(f\"‚úÖ Loaded: {path.name} ({len(team_stats):,} rows)\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {path}: {e}\")\n",
    "\n",
    "if team_stats is None:\n",
    "    print(\"‚ùå No game data found!\")\n",
    "    print(\"   Searched:\", [str(p) for p in possible_paths])\n",
    "else:\n",
    "    # ---------------------------------------------------------\n",
    "    # STEP: Robust date parsing (2-pass, all tz-aware -> tz-naive)\n",
    "    # ---------------------------------------------------------\n",
    "    date_col = None\n",
    "    for col in [\"gameDate\", \"Date\", \"date\", \"GAME_DATE\"]:\n",
    "        if col in team_stats.columns:\n",
    "            date_col = col\n",
    "            break\n",
    "\n",
    "    if not date_col:\n",
    "        print(\"‚ö†Ô∏è No date column found in team_stats!\")\n",
    "    else:\n",
    "        print(f\"\\nüìå Using date column: {date_col}\")\n",
    "\n",
    "        # Keep raw values for debugging\n",
    "        team_stats[\"gameDate_raw\"] = team_stats[date_col].astype(str)\n",
    "\n",
    "        # --- Pass 1: generic parser, force UTC (tz-aware) ---\n",
    "        parsed = pd.to_datetime(\n",
    "            team_stats[date_col],\n",
    "            errors=\"coerce\",   # unparseable -> NaT\n",
    "            utc=True,          # everything tz-aware\n",
    "        )\n",
    "        invalid_mask = parsed.isna()\n",
    "        invalid_count = int(invalid_mask.sum())\n",
    "        print(f\"‚ö†Ô∏è NaT after generic parse: {invalid_count}\")\n",
    "\n",
    "        # --- Pass 2: explicit format for older rows like '11/3/1995 20:00' ---\n",
    "        if invalid_count > 0:\n",
    "            alt_parsed = pd.to_datetime(\n",
    "                team_stats.loc[invalid_mask, date_col],\n",
    "                format=\"%m/%d/%Y %H:%M\",\n",
    "                errors=\"coerce\",\n",
    "                utc=True,  # also tz-aware\n",
    "            )\n",
    "            parsed.loc[invalid_mask] = alt_parsed\n",
    "\n",
    "            # Recompute invalids\n",
    "            invalid_mask = parsed.isna()\n",
    "            invalid_count = int(invalid_mask.sum())\n",
    "            print(f\"‚ö†Ô∏è Remaining NaT after m/d/Y H:M parse: {invalid_count}\")\n",
    "\n",
    "            if invalid_count > 0:\n",
    "                print(\"\\nüîç Sample of still-invalid 'gameDate_raw' values:\")\n",
    "                sample = (\n",
    "                    team_stats.loc[invalid_mask, \"gameDate_raw\"]\n",
    "                    .value_counts()\n",
    "                    .head(10)\n",
    "                )\n",
    "                print(sample)\n",
    "\n",
    "        # Now parsed is entirely tz-aware (where not NaT). Strip timezone -> tz-naive\n",
    "        parsed = parsed.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n",
    "        # Attach parsed dates\n",
    "        team_stats[\"gameDate\"] = parsed\n",
    "\n",
    "        # Drop truly invalid rows\n",
    "        valid_dates = team_stats[\"gameDate\"].notna()\n",
    "        dropped = int((~valid_dates).sum())\n",
    "        if dropped > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è Dropping {dropped} rows with unparseable dates after both passes.\")\n",
    "        team_stats = team_stats[valid_dates].copy()\n",
    "\n",
    "        if len(team_stats) > 0:\n",
    "            # sort by date now that everything is tz-naive\n",
    "            team_stats = team_stats.sort_values(\"gameDate\").reset_index(drop=True)\n",
    "\n",
    "            print(f\"\\n‚úÖ Remaining rows: {len(team_stats):,}\")\n",
    "            print(\n",
    "                \"üìÖ Date range:\",\n",
    "                team_stats[\"gameDate\"].min().date(),\n",
    "                \"to\",\n",
    "                team_stats[\"gameDate\"].max().date(),\n",
    "            )\n",
    "        else:\n",
    "            print(\"‚ùå No valid dates in data after parsing!\")\n",
    "\n",
    "    print(f\"\\nüìã Columns available: {list(team_stats.columns)[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Set Backtest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Backtest Configuration:\n",
      "   Start: 2025-05-21\n",
      "   End:   2025-11-17\n",
      "   Days:  180\n",
      "\n",
      "üìä Games in backtest window: 294\n",
      "‚úÖ Ready to backtest!\n"
     ]
    }
   ],
   "source": [
    "if team_stats is not None and 'gameDate' in team_stats.columns:\n",
    "    # Auto-detect date range from data\n",
    "    latest_date = team_stats['gameDate'].max()\n",
    "    earliest_date = team_stats['gameDate'].min()\n",
    "    \n",
    "    # Default: last 30 days of available data\n",
    "    BACKTEST_START = latest_date - timedelta(days=180)\n",
    "    BACKTEST_END = latest_date\n",
    "    \n",
    "    print(f\"üéØ Backtest Configuration:\")\n",
    "    print(f\"   Start: {BACKTEST_START.date()}\")\n",
    "    print(f\"   End:   {BACKTEST_END.date()}\")\n",
    "    print(f\"   Days:  {(BACKTEST_END - BACKTEST_START).days}\")\n",
    "    \n",
    "    # Filter to backtest window\n",
    "    backtest_data = team_stats[\n",
    "        (team_stats['gameDate'] >= BACKTEST_START) &\n",
    "        (team_stats['gameDate'] <= BACKTEST_END)\n",
    "    ].copy()\n",
    "    \n",
    "    # Get home games only (avoid duplicates)\n",
    "    if 'home' in backtest_data.columns:\n",
    "        backtest_games = backtest_data[backtest_data['home'] == 1].copy()\n",
    "    else:\n",
    "        backtest_games = backtest_data.copy()\n",
    "    \n",
    "    print(f\"\\nüìä Games in backtest window: {len(backtest_games)}\")\n",
    "    \n",
    "    # Create standardized team name columns\n",
    "    if 'teamName' in backtest_games.columns:\n",
    "        backtest_games['Home_Team'] = (backtest_games.get('teamCity', '') + ' ' + backtest_games['teamName']).str.strip()\n",
    "        backtest_games['Away_Team'] = (backtest_games.get('opponentTeamCity', '') + ' ' + backtest_games.get('opponentTeamName', '')).str.strip()\n",
    "    \n",
    "    # Create score columns\n",
    "    for src, dst in [('teamScore', 'Home_Score'), ('opponentScore', 'Away_Score')]:\n",
    "        if src in backtest_games.columns:\n",
    "            backtest_games[dst] = backtest_games[src]\n",
    "    \n",
    "    if len(backtest_games) > 0:\n",
    "        print(\"‚úÖ Ready to backtest!\")\n",
    "    else:\n",
    "        print(\"‚ùå No games found in date range\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot set parameters - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Run QEPC Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Running QEPC predictions...\n",
      "\n",
      "[Strengths] Loading game data from C:\\Users\\wdors\\qepc_project\\data\\raw\\TeamStatistics.csv\n",
      "Computed real lambdas for 1 games.\n",
      "[Strengths] Loading game data from C:\\Users\\wdors\\qepc_project\\data\\raw\\TeamStatistics.csv\n",
      "Computed real lambdas for 1 games.\n",
      "[Strengths] Loading game data from C:\\Users\\wdors\\qepc_project\\data\\raw\\TeamStatistics.csv\n",
      "Computed real lambdas for 1 games.\n",
      "[Strengths] Loading game data from C:\\Users\\wdors\\qepc_project\\data\\raw\\TeamStatistics.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m away_team \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway_Team\u001b[39m\u001b[38;5;124m'\u001b[39m, game\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopponentTeamName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get team strengths\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m strengths \u001b[38;5;241m=\u001b[39m calculate_advanced_strengths(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strengths\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     22\u001b[0m     errors_log\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: No strength data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\qepc_project\\qepc\\sports\\nba\\strengths_v2.py:221\u001b[0m, in \u001b[0;36mcalculate_advanced_strengths\u001b[1;34m(game_data, cutoff_date, verbose)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Keep original columns for debugging / sanity checks\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORtg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDRtg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPace\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 221\u001b[0m     hist_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hist_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m merged\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    223\u001b[0m         merged[hist_col] \u001b[38;5;241m=\u001b[39m merged[col]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cmp_method(other, operator\u001b[38;5;241m.\u001b[39meq)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:130\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mscalar_compare(x\u001b[38;5;241m.\u001b[39mravel(), y, op)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"üîÆ Running QEPC predictions...\\n\")\n",
    "\n",
    "results = []\n",
    "errors_log = []\n",
    "\n",
    "if 'backtest_games' in dir() and len(backtest_games) > 0:\n",
    "    total_games = len(backtest_games)\n",
    "    \n",
    "    for i, (idx, game) in enumerate(backtest_games.iterrows()):\n",
    "        # Progress indicator\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            print(f\"‚è≥ Processing game {i+1}/{total_games}...\", end=\"\\r\")\n",
    "        \n",
    "        try:\n",
    "            home_team = game.get('Home_Team', game.get('teamName', 'Home'))\n",
    "            away_team = game.get('Away_Team', game.get('opponentTeamName', 'Away'))\n",
    "            \n",
    "            # Get team strengths\n",
    "            strengths = calculate_advanced_strengths(verbose=False)\n",
    "            \n",
    "            if strengths.empty:\n",
    "                errors_log.append(f\"Game {i}: No strength data\")\n",
    "                continue\n",
    "            \n",
    "            # Build schedule\n",
    "            schedule = pd.DataFrame([{\n",
    "                'Home Team': home_team,\n",
    "                'Away Team': away_team\n",
    "            }])\n",
    "            \n",
    "            # Compute lambdas\n",
    "            schedule_with_lambda = compute_lambda(schedule, strengths)\n",
    "            \n",
    "            # Run simulation\n",
    "            predictions = run_qepc_simulation(schedule_with_lambda, num_trials=5000)\n",
    "            \n",
    "            if len(predictions) == 0:\n",
    "                continue\n",
    "            \n",
    "            pred = predictions.iloc[0]\n",
    "            \n",
    "            # Get predictions\n",
    "            pred_home = pred.get('Sim_Home_Score', pred.get('lambda_home', 110))\n",
    "            pred_away = pred.get('Sim_Away_Score', pred.get('lambda_away', 108))\n",
    "            home_win_prob = pred.get('Home_Win_Prob', 0.5)\n",
    "            \n",
    "            # Get actuals\n",
    "            actual_home = game.get('Home_Score', game.get('teamScore', 0))\n",
    "            actual_away = game.get('Away_Score', game.get('opponentScore', 0))\n",
    "            \n",
    "            # Calculate outcomes\n",
    "            actual_home_won = actual_home > actual_away\n",
    "            pred_home_won = home_win_prob > 0.5\n",
    "            \n",
    "            results.append({\n",
    "                'Date': game['gameDate'],\n",
    "                'Home_Team': home_team,\n",
    "                'Away_Team': away_team,\n",
    "                'Pred_Home_Score': round(pred_home, 1),\n",
    "                'Pred_Away_Score': round(pred_away, 1),\n",
    "                'Pred_Total': round(pred_home + pred_away, 1),\n",
    "                'Pred_Spread': round(pred_home - pred_away, 1),\n",
    "                'Home_Win_Prob': round(home_win_prob, 3),\n",
    "                'Actual_Home_Score': actual_home,\n",
    "                'Actual_Away_Score': actual_away,\n",
    "                'Actual_Total': actual_home + actual_away,\n",
    "                'Actual_Spread': actual_home - actual_away,\n",
    "                'Winner_Correct': actual_home_won == pred_home_won,\n",
    "                'Error_Total': abs((pred_home + pred_away) - (actual_home + actual_away)),\n",
    "                'Error_Spread': abs((pred_home - pred_away) - (actual_home - actual_away)),\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors_log.append(f\"Game {i}: {str(e)[:40]}\")\n",
    "    \n",
    "    print(\"\\n\")  # Clear progress line\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"‚úÖ Backtest complete!\")\n",
    "    print(f\"   Games analyzed: {len(results_df)}\")\n",
    "    print(f\"   Errors skipped: {len(errors_log)}\")\n",
    "else:\n",
    "    print(\"‚ùå No games to backtest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in dir() and len(results_df) > 0:\n",
    "    # Calculate metrics\n",
    "    win_accuracy = results_df['Winner_Correct'].mean()\n",
    "    avg_total_error = results_df['Error_Total'].mean()\n",
    "    avg_spread_error = results_df['Error_Spread'].mean()\n",
    "    median_total_error = results_df['Error_Total'].median()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä BACKTEST RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              PERFORMANCE SUMMARY                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Games Analyzed:     {len(results_df):>6}                    ‚îÇ\n",
    "‚îÇ  Win Accuracy:       {win_accuracy:>6.1%}                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Avg Total Error:    {avg_total_error:>6.1f} pts               ‚îÇ\n",
    "‚îÇ  Median Total Error: {median_total_error:>6.1f} pts               ‚îÇ\n",
    "‚îÇ  Avg Spread Error:   {avg_spread_error:>6.1f} pts               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \"\"\")\n",
    "    \n",
    "    # High confidence analysis\n",
    "    results_df['Confidence'] = abs(results_df['Pred_Spread'])\n",
    "    high_conf = results_df[results_df['Confidence'] > 5]\n",
    "    \n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\nüéØ High Confidence Games (|spread| > 5):\")\n",
    "        print(f\"   Count: {len(high_conf)}\")\n",
    "        print(f\"   Accuracy: {high_conf['Winner_Correct'].mean():.1%}\")\n",
    "    \n",
    "    # Best predictions\n",
    "    print(f\"\\nüèÜ Best Predictions (smallest error):\")\n",
    "    best = results_df.nsmallest(5, 'Error_Total')\n",
    "    for _, row in best.iterrows():\n",
    "        date = pd.Timestamp(row['Date']).strftime('%m-%d')\n",
    "        print(f\"   {date}: {row['Away_Team'][:18]:18} @ {row['Home_Team'][:18]:18} | Error: {row['Error_Total']:.1f}\")\n",
    "    \n",
    "    # Worst predictions\n",
    "    print(f\"\\n‚ö†Ô∏è Worst Predictions (largest error):\")\n",
    "    worst = results_df.nlargest(5, 'Error_Total')\n",
    "    for _, row in worst.iterrows():\n",
    "        date = pd.Timestamp(row['Date']).strftime('%m-%d')\n",
    "        print(f\"   {date}: {row['Away_Team'][:18]:18} @ {row['Home_Team'][:18]:18} | Error: {row['Error_Total']:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå No results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PLOTS and 'results_df' in dir() and len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Predicted vs Actual Total\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(results_df['Actual_Total'], results_df['Pred_Total'], alpha=0.6, s=50)\n",
    "    min_val = min(results_df['Actual_Total'].min(), results_df['Pred_Total'].min()) - 10\n",
    "    max_val = max(results_df['Actual_Total'].max(), results_df['Pred_Total'].max()) + 10\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "    ax1.set_xlabel('Actual Total', fontsize=12)\n",
    "    ax1.set_ylabel('Predicted Total', fontsize=12)\n",
    "    ax1.set_title('Predicted vs Actual Total Score', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Error Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(results_df['Error_Total'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax2.axvline(results_df['Error_Total'].mean(), color='r', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {avg_total_error:.1f}')\n",
    "    ax2.axvline(results_df['Error_Total'].median(), color='orange', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {median_total_error:.1f}')\n",
    "    ax2.set_xlabel('Total Error (points)', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Distribution of Total Score Error', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Error Over Time\n",
    "    ax3 = axes[1, 0]\n",
    "    results_sorted = results_df.sort_values('Date')\n",
    "    ax3.plot(range(len(results_sorted)), results_sorted['Error_Total'], \n",
    "             marker='o', alpha=0.6, markersize=5, linewidth=1)\n",
    "    ax3.axhline(avg_total_error, color='r', linestyle='--', linewidth=2, label='Mean Error')\n",
    "    ax3.set_xlabel('Game Number', fontsize=12)\n",
    "    ax3.set_ylabel('Total Error (points)', fontsize=12)\n",
    "    ax3.set_title('Prediction Error Over Time', fontsize=14)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Win Accuracy by Confidence\n",
    "    ax4 = axes[1, 1]\n",
    "    bins = [0, 3, 6, 10, 100]\n",
    "    labels = ['0-3', '3-6', '6-10', '10+']\n",
    "    results_df['Conf_Bin'] = pd.cut(results_df['Confidence'], bins=bins, labels=labels)\n",
    "    \n",
    "    accuracy_by_conf = results_df.groupby('Conf_Bin', observed=True)['Winner_Correct'].agg(['mean', 'count'])\n",
    "    \n",
    "    bars = ax4.bar(range(len(accuracy_by_conf)), accuracy_by_conf['mean'], color='steelblue')\n",
    "    ax4.axhline(0.5, color='r', linestyle='--', linewidth=2, label='50% (coin flip)')\n",
    "    ax4.axhline(win_accuracy, color='green', linestyle='--', linewidth=2, label=f'Overall: {win_accuracy:.1%}')\n",
    "    ax4.set_xticks(range(len(accuracy_by_conf)))\n",
    "    ax4.set_xticklabels(labels)\n",
    "    ax4.set_xlabel('Predicted Spread (confidence)', fontsize=12)\n",
    "    ax4.set_ylabel('Win Accuracy', fontsize=12)\n",
    "    ax4.set_title('Win Accuracy by Confidence Level', fontsize=14)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, accuracy_by_conf['count'])):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'n={int(count)}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations complete\")\n",
    "elif not HAS_PLOTS:\n",
    "    print(\"‚ö†Ô∏è Matplotlib not available - skipping visualizations\")\n",
    "else:\n",
    "    print(\"‚ùå No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in dir() and len(results_df) > 0:\n",
    "    # Save detailed results\n",
    "    output_dir = project_root / \"data\" / \"results\" / \"backtests\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"Enhanced_Backtest_{timestamp}.csv\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Saved results to: {output_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\"\"\n",
    "üìã FINAL SUMMARY\n",
    "================\n",
    "Period:       {BACKTEST_START.date()} to {BACKTEST_END.date()}\n",
    "Games:        {len(results_df)}\n",
    "Win Accuracy: {win_accuracy:.1%}\n",
    "Avg Error:    {avg_total_error:.1f} pts\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"‚ùå No results to save\")\n",
    "\n",
    "print(\"üèÅ Backtest complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Next Steps\n",
    "\n",
    "### Based on your results:\n",
    "\n",
    "**If Win Accuracy < 55%:**\n",
    "- Add recency weighting to team strengths\n",
    "- Include rest day adjustments\n",
    "- Consider injuries impact\n",
    "\n",
    "**If Total Error > 15 points:**\n",
    "- Calibrate lambda calculations\n",
    "- Add pace adjustments\n",
    "- Review team volatility modeling\n",
    "\n",
    "**If High Confidence games underperform:**\n",
    "- Add upset probability (quantum tunneling)\n",
    "- Consider travel factors\n",
    "- Review matchup-specific adjustments\n",
    "\n",
    "---\n",
    "\n",
    "**Use these insights to improve QEPC!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to get the true project root from QEPC's autoload paths module\n",
    "try:\n",
    "    from qepc.autoload.paths import get_project_root\n",
    "    project_root = get_project_root()\n",
    "except Exception:\n",
    "    # Fallback if that import fails for some reason\n",
    "    project_root = Path.cwd()\n",
    "    print(\"‚ö†Ô∏è Falling back to cwd as project root\")\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "# Helper: pick the \"best\" match for a file name among many\n",
    "def pick_best_match(matches):\n",
    "    if not matches:\n",
    "        return None\n",
    "    # Prefer paths that live under a 'data' folder and NOT under 'notebooks'\n",
    "    scored = []\n",
    "    for p in matches:\n",
    "        score = 0\n",
    "        parts = [str(part).lower() for part in p.parts]\n",
    "        if \"data\" in parts:\n",
    "            score += 2\n",
    "        if \"raw\" in parts:\n",
    "            score += 1\n",
    "        if \"props\" in parts:\n",
    "            score += 1\n",
    "        if \"results\" in parts:\n",
    "            score += 1\n",
    "        if \"notebooks\" in parts:\n",
    "            score -= 2\n",
    "        if \".ipynb_checkpoints\" in str(p):\n",
    "            score -= 5\n",
    "        scored.append((score, p))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return scored[0][1]\n",
    "\n",
    "# (label, filename)\n",
    "targets = [\n",
    "    # Core game/team data\n",
    "    (\"TeamStatistics (team game logs)\",      \"TeamStatistics.csv\"),\n",
    "    (\"Team_Stats (team season stats)\",       \"Team_Stats.csv\"),\n",
    "    (\"PlayerStatistics (player logs)\",       \"PlayerStatistics.csv\"),\n",
    "    (\"Canonical Games (schedule)\",           \"Games.csv\"),\n",
    "    (\"GameResults_2025 (results)\",           \"GameResults_2025.csv\"),\n",
    "    (\"Schedule_with_Rest\",                   \"Schedule_with_Rest.csv\"),\n",
    "    (\"TeamForm\",                             \"TeamForm.csv\"),\n",
    "\n",
    "    # Roster / players\n",
    "    (\"Players\",                              \"Players.csv\"),\n",
    "    (\"Players_Processed\",                    \"Players_Processed.csv\"),\n",
    "\n",
    "    # Injuries\n",
    "    (\"Injury_Overrides\",                     \"Injury_Overrides.csv\"),\n",
    "    (\"Injury_Overrides_MASTER\",              \"Injury_Overrides_MASTER.csv\"),\n",
    "    (\"Injury_Overrides_live_espn\",           \"Injury_Overrides_live_espn.csv\"),\n",
    "\n",
    "    # Props / aggregates\n",
    "    (\"Player_Season_Averages\",               \"Player_Season_Averages.csv\"),\n",
    "    (\"Player_Averages_With_CI\",              \"Player_Averages_With_CI.csv\"),\n",
    "    (\"Player_Recent_Form_L5\",                \"Player_Recent_Form_L5.csv\"),\n",
    "    (\"Player_Recent_Form_L10\",               \"Player_Recent_Form_L10.csv\"),\n",
    "    (\"Player_Recent_Form_L15\",               \"Player_Recent_Form_L15.csv\"),\n",
    "    (\"Player_Home_Away_Splits\",              \"Player_Home_Away_Splits.csv\"),\n",
    "]\n",
    "\n",
    "def preview_by_filename(label: str, filename: str, n: int = 3):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìÑ {label}\")\n",
    "    print(f\"Looking for filename: {filename}\")\n",
    "\n",
    "    # Find all matches anywhere under project_root\n",
    "    matches = [p for p in project_root.rglob(filename)]\n",
    "    if not matches:\n",
    "        print(\"‚ö†Ô∏è No matches found in project.\")\n",
    "        return\n",
    "\n",
    "    print(\"Found matches:\")\n",
    "    for m in matches:\n",
    "        try:\n",
    "            rel = m.relative_to(project_root)\n",
    "        except ValueError:\n",
    "            rel = m\n",
    "        print(\"   ‚Ä¢\", rel)\n",
    "\n",
    "    best = pick_best_match(matches)\n",
    "    if best is None:\n",
    "        print(\"‚ö†Ô∏è Could not choose a best match.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        rel_best = best.relative_to(project_root)\n",
    "    except ValueError:\n",
    "        rel_best = best\n",
    "\n",
    "    print(f\"\\n‚úÖ Using best match: {rel_best}\")\n",
    "\n",
    "    # Load a small sample (nrows=3) to avoid pulling full 300MB files\n",
    "    try:\n",
    "        df_sample = pd.read_csv(best, nrows=n)\n",
    "        print(f\"Sample shape: {df_sample.shape}\")\n",
    "        print(\"Columns:\", list(df_sample.columns))\n",
    "        print(\"\\nSample rows:\")\n",
    "        display(df_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading CSV sample: {e}\")\n",
    "\n",
    "for label, filename in targets:\n",
    "    preview_by_filename(label, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# QEPC imports\n",
    "from qepc.sports.nba.strengths_v2 import calculate_advanced_strengths\n",
    "from qepc.core.lambda_engine import compute_lambda\n",
    "from qepc.core.simulator import run_qepc_simulation\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0) Resolve data paths (project_root / data / raw)\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    from qepc.autoload.paths import get_data_dir, get_raw_data_dir\n",
    "\n",
    "    data_dir = get_data_dir()\n",
    "    raw_dir = get_raw_data_dir()\n",
    "except ImportError:\n",
    "    # Fallback if get_raw_data_dir doesn't exist\n",
    "    from qepc.autoload.paths import get_data_dir\n",
    "\n",
    "    data_dir = get_data_dir()\n",
    "    raw_dir = data_dir / \"raw\"\n",
    "except Exception:\n",
    "    # Last-resort fallback\n",
    "    project_root = Path.cwd().parents[0]\n",
    "    data_dir = project_root / \"data\"\n",
    "    raw_dir = data_dir / \"raw\"\n",
    "\n",
    "print(\"[QEPC] data_dir:\", data_dir)\n",
    "print(\"[QEPC] raw_dir:\", raw_dir)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Load TeamStatistics.csv and parse gameDate robustly\n",
    "# ---------------------------------------------------------\n",
    "team_stats_path = raw_dir / \"TeamStatistics.csv\"\n",
    "print(\"\\nüìä Loading team game logs from:\", team_stats_path)\n",
    "\n",
    "if not team_stats_path.exists():\n",
    "    raise FileNotFoundError(f\"TeamStatistics.csv not found at {team_stats_path}\")\n",
    "\n",
    "team_stats_raw = pd.read_csv(team_stats_path)\n",
    "print(f\"‚úÖ Loaded TeamStatistics: {len(team_stats_raw):,} rows, {len(team_stats_raw.columns)} columns\")\n",
    "\n",
    "# Detect date column\n",
    "date_col = None\n",
    "for col in [\"gameDate\", \"Date\", \"date\", \"GAME_DATE\"]:\n",
    "    if col in team_stats_raw.columns:\n",
    "        date_col = col\n",
    "        break\n",
    "\n",
    "if date_col is None:\n",
    "    raise RuntimeError(\"No date-like column found in TeamStatistics.csv\")\n",
    "\n",
    "print(f\"üìå Using date column: {date_col}\")\n",
    "\n",
    "team_stats = team_stats_raw.copy()\n",
    "team_stats[\"gameDate_raw\"] = team_stats[date_col].astype(str)\n",
    "\n",
    "# Pass 1: generic parser, tz-aware\n",
    "parsed = pd.to_datetime(\n",
    "    team_stats[date_col],\n",
    "    errors=\"coerce\",\n",
    "    utc=True,\n",
    ")\n",
    "invalid_mask = parsed.isna()\n",
    "invalid_count = int(invalid_mask.sum())\n",
    "print(f\"‚ö†Ô∏è NaT after generic parse: {invalid_count}\")\n",
    "\n",
    "# Pass 2: explicit \"%m/%d/%Y %H:%M\" for old records (also tz-aware)\n",
    "if invalid_count > 0:\n",
    "    alt_parsed = pd.to_datetime(\n",
    "        team_stats.loc[invalid_mask, date_col],\n",
    "        format=\"%m/%d/%Y %H:%M\",\n",
    "        errors=\"coerce\",\n",
    "        utc=True,\n",
    "    )\n",
    "    parsed.loc[invalid_mask] = alt_parsed\n",
    "    invalid_mask = parsed.isna()\n",
    "    invalid_count = int(invalid_mask.sum())\n",
    "    print(f\"‚ö†Ô∏è Remaining NaT after m/d/Y H:M parse: {invalid_count}\")\n",
    "\n",
    "    if invalid_count > 0:\n",
    "        print(\"\\nüîç Sample of still-invalid 'gameDate_raw' values:\")\n",
    "        sample = (\n",
    "            team_stats.loc[invalid_mask, \"gameDate_raw\"]\n",
    "            .value_counts()\n",
    "            .head(10)\n",
    "        )\n",
    "        print(sample)\n",
    "\n",
    "# Strip timezone ‚Üí tz-naive\n",
    "parsed = parsed.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "team_stats[\"gameDate\"] = parsed\n",
    "\n",
    "# Drop rows with no valid date\n",
    "valid_dates = team_stats[\"gameDate\"].notna()\n",
    "dropped = int((~valid_dates).sum())\n",
    "if dropped > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Dropping {dropped} rows with unparseable dates after both passes.\")\n",
    "team_stats = team_stats[valid_dates].copy()\n",
    "\n",
    "# Sort by date\n",
    "team_stats = team_stats.sort_values(\"gameDate\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Final TeamStatistics rows: {len(team_stats):,}\")\n",
    "print(\n",
    "    \"üìÖ Date range:\",\n",
    "    team_stats[\"gameDate\"].min().date(),\n",
    "    \"to\",\n",
    "    team_stats[\"gameDate\"].max().date(),\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Build recent evaluation set (one row per game)\n",
    "# ---------------------------------------------------------\n",
    "recent_cutoff = pd.Timestamp(\"2024-10-01\")\n",
    "recent_games = team_stats[team_stats[\"gameDate\"] >= recent_cutoff].copy()\n",
    "\n",
    "print(f\"\\nüïí Recent games since {recent_cutoff.date()}: {len(recent_games):,} team-rows\")\n",
    "\n",
    "# Collapse to one row per game by keeping one of the pair.\n",
    "# NOTE: This uses a deterministic rule (teamName < opponentTeamName)\n",
    "# just to avoid duplicates. For totals calibration, home/away doesn't matter.\n",
    "mask_keep = recent_games[\"teamName\"] < recent_games[\"opponentTeamName\"]\n",
    "games_eval = recent_games[mask_keep].copy()\n",
    "\n",
    "games_eval = games_eval.rename(\n",
    "    columns={\n",
    "        \"teamName\": \"Home Team\",          # label only; not true home\n",
    "        \"opponentTeamName\": \"Away Team\",  # label only; not true away\n",
    "        \"teamScore\": \"Home_Score\",\n",
    "        \"opponentScore\": \"Away_Score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"üéØ Unique game-rows for evaluation (calibration): {len(games_eval):,}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Compute team strengths from full game log\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n‚ßâ QEPC: Computing team strengths (calculate_advanced_strengths)...\")\n",
    "strengths_df = calculate_advanced_strengths(\n",
    "    game_data=team_stats,\n",
    "    cutoff_date=None,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "if strengths_df is None or strengths_df.empty:\n",
    "    raise RuntimeError(\"strengths_df is empty. Check strengths_v2 configuration.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Build schedule df and compute lambdas\n",
    "# ---------------------------------------------------------\n",
    "schedule_df = games_eval[[\"Home Team\", \"Away Team\"]].reset_index(drop=True)\n",
    "\n",
    "lambda_df = compute_lambda(\n",
    "    schedule_df=schedule_df,\n",
    "    team_stats_df=strengths_df,\n",
    "    include_situational=False,  # keep it clean for calibration\n",
    ")\n",
    "\n",
    "print(f\"Computed lambdas for {len(lambda_df):,} games.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Run QEPC simulation on these lambdas\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n‚ßâ QEPC: Running QEPC simulation on calibration sample...\")\n",
    "sim_results = run_qepc_simulation(\n",
    "    df=lambda_df,\n",
    "    num_trials=3000,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Assemble eval_df: actual vs predicted totals & margins\n",
    "# ---------------------------------------------------------\n",
    "eval_df = games_eval.reset_index(drop=True).copy()\n",
    "\n",
    "# Attach lambdas & sim outputs\n",
    "eval_df[\"lambda_home\"] = lambda_df[\"lambda_home\"].values\n",
    "eval_df[\"lambda_away\"] = lambda_df[\"lambda_away\"].values\n",
    "eval_df[\"Sim_Home_Score\"] = sim_results[\"Sim_Home_Score\"].values\n",
    "eval_df[\"Sim_Away_Score\"] = sim_results[\"Sim_Away_Score\"].values\n",
    "eval_df[\"Home_Win_Prob\"] = sim_results[\"Home_Win_Prob\"].values\n",
    "eval_df[\"Expected_Score_Total\"] = sim_results[\"Expected_Score_Total\"].values\n",
    "\n",
    "# Actual totals / margins\n",
    "eval_df[\"Actual_Total\"] = eval_df[\"Home_Score\"] + eval_df[\"Away_Score\"]\n",
    "eval_df[\"Actual_Margin\"] = eval_df[\"Home_Score\"] - eval_df[\"Away_Score\"]\n",
    "\n",
    "# Predicted totals / margins\n",
    "eval_df[\"Pred_Total\"] = eval_df[\"Expected_Score_Total\"]\n",
    "eval_df[\"Pred_Margin\"] = eval_df[\"Sim_Home_Score\"] - eval_df[\"Sim_Away_Score\"]\n",
    "\n",
    "# Binary outcome (based on fake \"home\" label; totals calibration doesn't care)\n",
    "eval_df[\"Actual_Home_Win\"] = (eval_df[\"Home_Score\"] > eval_df[\"Away_Score\"]).astype(int)\n",
    "eval_df[\"Pred_Home_Win\"] = (eval_df[\"Home_Win_Prob\"] >= 0.5).astype(int)\n",
    "\n",
    "# Errors\n",
    "eval_df[\"Total_Error\"] = (eval_df[\"Pred_Total\"] - eval_df[\"Actual_Total\"]).abs()\n",
    "eval_df[\"Spread_Error\"] = (eval_df[\"Pred_Margin\"] - eval_df[\"Actual_Margin\"]).abs()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Summary metrics\n",
    "# ---------------------------------------------------------\n",
    "n_games = len(eval_df)\n",
    "win_acc = (eval_df[\"Pred_Home_Win\"] == eval_df[\"Actual_Home_Win\"]).mean() * 100.0\n",
    "mae_total = eval_df[\"Total_Error\"].mean()\n",
    "med_total = eval_df[\"Total_Error\"].median()\n",
    "mae_spread = eval_df[\"Spread_Error\"].mean()\n",
    "\n",
    "print(\"\\nüìä MINI BACKTEST SUMMARY (CALIBRATION SAMPLE)\")\n",
    "print(\"============================================================\")\n",
    "print(f\"Games Evaluated:       {n_games}\")\n",
    "print(f\"Home-Win Accuracy:     {win_acc:5.1f}%  (NOTE: 'home' label here is synthetic)\")\n",
    "print(f\"Mean Total Error:      {mae_total:5.1f} pts\")\n",
    "print(f\"Median Total Error:    {med_total:5.1f} pts\")\n",
    "print(f\"Mean Spread Error:     {mae_spread:5.1f} pts\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) Compute calibration factor for Œª totals\n",
    "# ---------------------------------------------------------\n",
    "mean_actual_total = eval_df[\"Actual_Total\"].mean()\n",
    "mean_pred_total = eval_df[\"Pred_Total\"].mean()\n",
    "calib_factor = mean_actual_total / mean_pred_total\n",
    "\n",
    "print(\"\\nüéØ CALIBRATION FACTOR\")\n",
    "print(\"============================================================\")\n",
    "print(f\"Mean actual total:     {mean_actual_total:.2f}\")\n",
    "print(f\"Mean predicted total:  {mean_pred_total:.2f}\")\n",
    "print(f\"Suggested LAMBDA_TOTAL_SCALE: {calib_factor:.4f}\")\n",
    "\n",
    "print(\"\\nüîé Sample rows (Actual vs Pred totals):\")\n",
    "display(\n",
    "    eval_df[\n",
    "        [\n",
    "            \"gameDate\",\n",
    "            \"Home Team\",\n",
    "            \"Away Team\",\n",
    "            \"Home_Score\",\n",
    "            \"Away_Score\",\n",
    "            \"Actual_Total\",\n",
    "            \"Pred_Total\",\n",
    "            \"Total_Error\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
