{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ QEPC Enhanced Backtest - FIXED\n",
    "\n",
    "This notebook:\n",
    "1. Uses **actual game results** from your data files\n",
    "2. Compares QEPC predictions to real outcomes\n",
    "3. Calculates detailed accuracy metrics\n",
    "4. Generates visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Project root: C:\\Users\\wdors\\qepc_project\n",
      "‚úÖ Matplotlib loaded\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# SETUP - This replaces the broken 'from notebook_context import *'\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root (works from any notebook location)\n",
    "current = Path.cwd()\n",
    "project_root = None\n",
    "\n",
    "for p in [current] + list(current.parents):\n",
    "    if (p / \"data\").exists() and (p / \"qepc\").exists():\n",
    "        project_root = p\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    project_root = current.parent.parent\n",
    "    \n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "\n",
    "# Add to Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization imports\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    HAS_PLOTS = True\n",
    "    print(\"‚úÖ Matplotlib loaded\")\n",
    "except ImportError:\n",
    "    HAS_PLOTS = False\n",
    "    print(\"‚ö†Ô∏è Matplotlib not available\")\n",
    "\n",
    "# QEPC imports - CORRECTED PATHS\n",
    "from qepc.sports.nba.strengths_v2 import calculate_advanced_strengths\n",
    "from qepc.core.lambda_engine import compute_lambda\n",
    "from qepc.core.simulator import run_qepc_simulation\n",
    "\n",
    "# Paths\n",
    "data_dir = project_root / \"data\"\n",
    "raw_dir = data_dir / \"raw\"\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Load Actual Game Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Looking for game data...\n",
      "‚úÖ Loaded: TeamStatistics.csv (144,314 rows)\n",
      "‚ö†Ô∏è Dropped 143758 rows with invalid dates\n",
      "\n",
      "üìÖ Date range: 2025-10-02 to 2025-11-17\n",
      "\n",
      "üìã Columns available: ['gameDate', 'teamCity', 'teamName', 'opponentTeamCity', 'opponentTeamName', 'teamScore', 'opponentScore', 'reboundsTotal', 'assists', 'threePointersMade']...\n"
     ]
    }
   ],
   "source": [
    "# Load game data\n",
    "print(\"üìä Looking for game data...\")\n",
    "\n",
    "possible_paths = [\n",
    "    raw_dir / \"TeamStatistics.csv\",\n",
    "    data_dir / \"TeamStatistics.csv\",\n",
    "    data_dir / \"GameResults_2025.csv\",\n",
    "    data_dir / \"Games.csv\",\n",
    "]\n",
    "\n",
    "team_stats = None\n",
    "for path in possible_paths:\n",
    "    if path.exists():\n",
    "        try:\n",
    "            team_stats = pd.read_csv(path)\n",
    "            print(f\"‚úÖ Loaded: {path.name} ({len(team_stats):,} rows)\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error with {path.name}: {e}\")\n",
    "\n",
    "if team_stats is None:\n",
    "    print(\"‚ùå No game data found!\")\n",
    "    print(f\"   Searched: {[p.name for p in possible_paths]}\")\n",
    "else:\n",
    "    # Parse dates\n",
    "    date_col = None\n",
    "    for col in ['gameDate', 'Date', 'date', 'GAME_DATE']:\n",
    "        if col in team_stats.columns:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        team_stats['gameDate'] = pd.to_datetime(team_stats[date_col], errors='coerce')\n",
    "        \n",
    "        # Drop rows with invalid dates\n",
    "        valid_dates = team_stats['gameDate'].notna()\n",
    "        invalid_count = (~valid_dates).sum()\n",
    "        \n",
    "        if invalid_count > 0:\n",
    "            print(f\"‚ö†Ô∏è Dropped {invalid_count} rows with invalid dates\")\n",
    "            team_stats = team_stats[valid_dates].copy()\n",
    "        \n",
    "        if len(team_stats) > 0:\n",
    "            print(f\"\\nüìÖ Date range: {team_stats['gameDate'].min().date()} to {team_stats['gameDate'].max().date()}\")\n",
    "        else:\n",
    "            print(\"‚ùå No valid dates in data!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No date column found\")\n",
    "    \n",
    "    print(f\"\\nüìã Columns available: {list(team_stats.columns)[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Set Backtest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Backtest Configuration:\n",
      "   Start: 2025-10-18\n",
      "   End:   2025-11-17\n",
      "   Days:  30\n",
      "\n",
      "üìä Games in backtest window: 207\n",
      "‚úÖ Ready to backtest!\n"
     ]
    }
   ],
   "source": [
    "if team_stats is not None and 'gameDate' in team_stats.columns:\n",
    "    # Auto-detect date range from data\n",
    "    latest_date = team_stats['gameDate'].max()\n",
    "    earliest_date = team_stats['gameDate'].min()\n",
    "    \n",
    "    # Default: last 30 days of available data\n",
    "    BACKTEST_START = latest_date - timedelta(days=30)\n",
    "    BACKTEST_END = latest_date\n",
    "    \n",
    "    print(f\"üéØ Backtest Configuration:\")\n",
    "    print(f\"   Start: {BACKTEST_START.date()}\")\n",
    "    print(f\"   End:   {BACKTEST_END.date()}\")\n",
    "    print(f\"   Days:  {(BACKTEST_END - BACKTEST_START).days}\")\n",
    "    \n",
    "    # Filter to backtest window\n",
    "    backtest_data = team_stats[\n",
    "        (team_stats['gameDate'] >= BACKTEST_START) &\n",
    "        (team_stats['gameDate'] <= BACKTEST_END)\n",
    "    ].copy()\n",
    "    \n",
    "    # Get home games only (avoid duplicates)\n",
    "    if 'home' in backtest_data.columns:\n",
    "        backtest_games = backtest_data[backtest_data['home'] == 1].copy()\n",
    "    else:\n",
    "        backtest_games = backtest_data.copy()\n",
    "    \n",
    "    print(f\"\\nüìä Games in backtest window: {len(backtest_games)}\")\n",
    "    \n",
    "    # Create standardized team name columns\n",
    "    if 'teamName' in backtest_games.columns:\n",
    "        backtest_games['Home_Team'] = (backtest_games.get('teamCity', '') + ' ' + backtest_games['teamName']).str.strip()\n",
    "        backtest_games['Away_Team'] = (backtest_games.get('opponentTeamCity', '') + ' ' + backtest_games.get('opponentTeamName', '')).str.strip()\n",
    "    \n",
    "    # Create score columns\n",
    "    for src, dst in [('teamScore', 'Home_Score'), ('opponentScore', 'Away_Score')]:\n",
    "        if src in backtest_games.columns:\n",
    "            backtest_games[dst] = backtest_games[src]\n",
    "    \n",
    "    if len(backtest_games) > 0:\n",
    "        print(\"‚úÖ Ready to backtest!\")\n",
    "    else:\n",
    "        print(\"‚ùå No games found in date range\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot set parameters - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Run QEPC Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Running QEPC predictions...\n",
      "\n",
      "‚è≥ Processing game 200/207...\n",
      "\n",
      "‚úÖ Backtest complete!\n",
      "   Games analyzed: 0\n",
      "   Errors skipped: 207\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÆ Running QEPC predictions...\\n\")\n",
    "\n",
    "results = []\n",
    "errors_log = []\n",
    "\n",
    "if 'backtest_games' in dir() and len(backtest_games) > 0:\n",
    "    total_games = len(backtest_games)\n",
    "    \n",
    "    for i, (idx, game) in enumerate(backtest_games.iterrows()):\n",
    "        # Progress indicator\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            print(f\"‚è≥ Processing game {i+1}/{total_games}...\", end=\"\\r\")\n",
    "        \n",
    "        try:\n",
    "            home_team = game.get('Home_Team', game.get('teamName', 'Home'))\n",
    "            away_team = game.get('Away_Team', game.get('opponentTeamName', 'Away'))\n",
    "            \n",
    "            # Get team strengths\n",
    "            strengths = calculate_advanced_strengths(verbose=False)\n",
    "            \n",
    "            if strengths.empty:\n",
    "                errors_log.append(f\"Game {i}: No strength data\")\n",
    "                continue\n",
    "            \n",
    "            # Build schedule\n",
    "            schedule = pd.DataFrame([{\n",
    "                'Home Team': home_team,\n",
    "                'Away Team': away_team\n",
    "            }])\n",
    "            \n",
    "            # Compute lambdas\n",
    "            schedule_with_lambda = compute_lambda(schedule, strengths)\n",
    "            \n",
    "            # Run simulation\n",
    "            predictions = run_qepc_simulation(schedule_with_lambda, num_trials=5000)\n",
    "            \n",
    "            if len(predictions) == 0:\n",
    "                continue\n",
    "            \n",
    "            pred = predictions.iloc[0]\n",
    "            \n",
    "            # Get predictions\n",
    "            pred_home = pred.get('Sim_Home_Score', pred.get('lambda_home', 110))\n",
    "            pred_away = pred.get('Sim_Away_Score', pred.get('lambda_away', 108))\n",
    "            home_win_prob = pred.get('Home_Win_Prob', 0.5)\n",
    "            \n",
    "            # Get actuals\n",
    "            actual_home = game.get('Home_Score', game.get('teamScore', 0))\n",
    "            actual_away = game.get('Away_Score', game.get('opponentScore', 0))\n",
    "            \n",
    "            # Calculate outcomes\n",
    "            actual_home_won = actual_home > actual_away\n",
    "            pred_home_won = home_win_prob > 0.5\n",
    "            \n",
    "            results.append({\n",
    "                'Date': game['gameDate'],\n",
    "                'Home_Team': home_team,\n",
    "                'Away_Team': away_team,\n",
    "                'Pred_Home_Score': round(pred_home, 1),\n",
    "                'Pred_Away_Score': round(pred_away, 1),\n",
    "                'Pred_Total': round(pred_home + pred_away, 1),\n",
    "                'Pred_Spread': round(pred_home - pred_away, 1),\n",
    "                'Home_Win_Prob': round(home_win_prob, 3),\n",
    "                'Actual_Home_Score': actual_home,\n",
    "                'Actual_Away_Score': actual_away,\n",
    "                'Actual_Total': actual_home + actual_away,\n",
    "                'Actual_Spread': actual_home - actual_away,\n",
    "                'Winner_Correct': actual_home_won == pred_home_won,\n",
    "                'Error_Total': abs((pred_home + pred_away) - (actual_home + actual_away)),\n",
    "                'Error_Spread': abs((pred_home - pred_away) - (actual_home - actual_away)),\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors_log.append(f\"Game {i}: {str(e)[:40]}\")\n",
    "    \n",
    "    print(\"\\n\")  # Clear progress line\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"‚úÖ Backtest complete!\")\n",
    "    print(f\"   Games analyzed: {len(results_df)}\")\n",
    "    print(f\"   Errors skipped: {len(errors_log)}\")\n",
    "else:\n",
    "    print(\"‚ùå No games to backtest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No results to analyze\n"
     ]
    }
   ],
   "source": [
    "if 'results_df' in dir() and len(results_df) > 0:\n",
    "    # Calculate metrics\n",
    "    win_accuracy = results_df['Winner_Correct'].mean()\n",
    "    avg_total_error = results_df['Error_Total'].mean()\n",
    "    avg_spread_error = results_df['Error_Spread'].mean()\n",
    "    median_total_error = results_df['Error_Total'].median()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä BACKTEST RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              PERFORMANCE SUMMARY                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Games Analyzed:     {len(results_df):>6}                    ‚îÇ\n",
    "‚îÇ  Win Accuracy:       {win_accuracy:>6.1%}                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Avg Total Error:    {avg_total_error:>6.1f} pts               ‚îÇ\n",
    "‚îÇ  Median Total Error: {median_total_error:>6.1f} pts               ‚îÇ\n",
    "‚îÇ  Avg Spread Error:   {avg_spread_error:>6.1f} pts               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \"\"\")\n",
    "    \n",
    "    # High confidence analysis\n",
    "    results_df['Confidence'] = abs(results_df['Pred_Spread'])\n",
    "    high_conf = results_df[results_df['Confidence'] > 5]\n",
    "    \n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\nüéØ High Confidence Games (|spread| > 5):\")\n",
    "        print(f\"   Count: {len(high_conf)}\")\n",
    "        print(f\"   Accuracy: {high_conf['Winner_Correct'].mean():.1%}\")\n",
    "    \n",
    "    # Best predictions\n",
    "    print(f\"\\nüèÜ Best Predictions (smallest error):\")\n",
    "    best = results_df.nsmallest(5, 'Error_Total')\n",
    "    for _, row in best.iterrows():\n",
    "        date = pd.Timestamp(row['Date']).strftime('%m-%d')\n",
    "        print(f\"   {date}: {row['Away_Team'][:18]:18} @ {row['Home_Team'][:18]:18} | Error: {row['Error_Total']:.1f}\")\n",
    "    \n",
    "    # Worst predictions\n",
    "    print(f\"\\n‚ö†Ô∏è Worst Predictions (largest error):\")\n",
    "    worst = results_df.nlargest(5, 'Error_Total')\n",
    "    for _, row in worst.iterrows():\n",
    "        date = pd.Timestamp(row['Date']).strftime('%m-%d')\n",
    "        print(f\"   {date}: {row['Away_Team'][:18]:18} @ {row['Home_Team'][:18]:18} | Error: {row['Error_Total']:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"‚ùå No results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data to visualize\n"
     ]
    }
   ],
   "source": [
    "if HAS_PLOTS and 'results_df' in dir() and len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Predicted vs Actual Total\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(results_df['Actual_Total'], results_df['Pred_Total'], alpha=0.6, s=50)\n",
    "    min_val = min(results_df['Actual_Total'].min(), results_df['Pred_Total'].min()) - 10\n",
    "    max_val = max(results_df['Actual_Total'].max(), results_df['Pred_Total'].max()) + 10\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "    ax1.set_xlabel('Actual Total', fontsize=12)\n",
    "    ax1.set_ylabel('Predicted Total', fontsize=12)\n",
    "    ax1.set_title('Predicted vs Actual Total Score', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Error Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(results_df['Error_Total'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax2.axvline(results_df['Error_Total'].mean(), color='r', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {avg_total_error:.1f}')\n",
    "    ax2.axvline(results_df['Error_Total'].median(), color='orange', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {median_total_error:.1f}')\n",
    "    ax2.set_xlabel('Total Error (points)', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Distribution of Total Score Error', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Error Over Time\n",
    "    ax3 = axes[1, 0]\n",
    "    results_sorted = results_df.sort_values('Date')\n",
    "    ax3.plot(range(len(results_sorted)), results_sorted['Error_Total'], \n",
    "             marker='o', alpha=0.6, markersize=5, linewidth=1)\n",
    "    ax3.axhline(avg_total_error, color='r', linestyle='--', linewidth=2, label='Mean Error')\n",
    "    ax3.set_xlabel('Game Number', fontsize=12)\n",
    "    ax3.set_ylabel('Total Error (points)', fontsize=12)\n",
    "    ax3.set_title('Prediction Error Over Time', fontsize=14)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Win Accuracy by Confidence\n",
    "    ax4 = axes[1, 1]\n",
    "    bins = [0, 3, 6, 10, 100]\n",
    "    labels = ['0-3', '3-6', '6-10', '10+']\n",
    "    results_df['Conf_Bin'] = pd.cut(results_df['Confidence'], bins=bins, labels=labels)\n",
    "    \n",
    "    accuracy_by_conf = results_df.groupby('Conf_Bin', observed=True)['Winner_Correct'].agg(['mean', 'count'])\n",
    "    \n",
    "    bars = ax4.bar(range(len(accuracy_by_conf)), accuracy_by_conf['mean'], color='steelblue')\n",
    "    ax4.axhline(0.5, color='r', linestyle='--', linewidth=2, label='50% (coin flip)')\n",
    "    ax4.axhline(win_accuracy, color='green', linestyle='--', linewidth=2, label=f'Overall: {win_accuracy:.1%}')\n",
    "    ax4.set_xticks(range(len(accuracy_by_conf)))\n",
    "    ax4.set_xticklabels(labels)\n",
    "    ax4.set_xlabel('Predicted Spread (confidence)', fontsize=12)\n",
    "    ax4.set_ylabel('Win Accuracy', fontsize=12)\n",
    "    ax4.set_title('Win Accuracy by Confidence Level', fontsize=14)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, accuracy_by_conf['count'])):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'n={int(count)}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations complete\")\n",
    "elif not HAS_PLOTS:\n",
    "    print(\"‚ö†Ô∏è Matplotlib not available - skipping visualizations\")\n",
    "else:\n",
    "    print(\"‚ùå No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in dir() and len(results_df) > 0:\n",
    "    # Save detailed results\n",
    "    output_dir = project_root / \"data\" / \"results\" / \"backtests\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"Enhanced_Backtest_{timestamp}.csv\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Saved results to: {output_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\"\"\n",
    "üìã FINAL SUMMARY\n",
    "================\n",
    "Period:       {BACKTEST_START.date()} to {BACKTEST_END.date()}\n",
    "Games:        {len(results_df)}\n",
    "Win Accuracy: {win_accuracy:.1%}\n",
    "Avg Error:    {avg_total_error:.1f} pts\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"‚ùå No results to save\")\n",
    "\n",
    "print(\"üèÅ Backtest complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Next Steps\n",
    "\n",
    "### Based on your results:\n",
    "\n",
    "**If Win Accuracy < 55%:**\n",
    "- Add recency weighting to team strengths\n",
    "- Include rest day adjustments\n",
    "- Consider injuries impact\n",
    "\n",
    "**If Total Error > 15 points:**\n",
    "- Calibrate lambda calculations\n",
    "- Add pace adjustments\n",
    "- Review team volatility modeling\n",
    "\n",
    "**If High Confidence games underperform:**\n",
    "- Add upset probability (quantum tunneling)\n",
    "- Consider travel factors\n",
    "- Review matchup-specific adjustments\n",
    "\n",
    "---\n",
    "\n",
    "**Use these insights to improve QEPC!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
