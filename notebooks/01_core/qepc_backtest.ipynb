{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9c713d-8d9c-4abe-9025-f8b8f48bc36c",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üìä QEPC NBA Backtest Notebook\n",
    "\n",
    "This notebook measures how well the QEPC NBA engine is performing over time.\n",
    "\n",
    "It focuses on:\n",
    "- üóìÔ∏è Defining a backtest date range\n",
    "- üßÆ Running `run_season_backtest(...)` to simulate all games in that range\n",
    "- üìà Summarizing accuracy & spread error\n",
    "- üß® Inspecting the worst misses\n",
    "- üßπ Filtering to NBA‚Äìvs‚ÄìNBA only (no exhibitions)\n",
    "- üåå (Optional) Script / total-error analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aafe66-976b-4071-8403-3aac9a7400db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28ab68-bb09-4a38-8501-be0364942fa5",
   "metadata": {},
   "source": [
    "## üß© 1. Environment & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5ab958-2b51-4107-ad76-a8cddf655d72",
   "metadata": {
    "editable": false,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QEPC Paths] Project Root set: C:\\Users\\wdors\\qepc_project\n",
      "[QEPC] Autoload complete.\n",
      "[QEPC] Root Shim Restored. Forwarding to qepc.autoload...\n",
      "Project root: C:\\Users\\wdors\\qepc_project\n"
     ]
    }
   ],
   "source": [
    "# Universal QEPC header for this notebook\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from notebook_context import *  # try direct import first\n",
    "except ModuleNotFoundError:\n",
    "    cwd = Path.cwd()\n",
    "    candidate_roots = [cwd, cwd.parent, cwd.parent.parent]\n",
    "\n",
    "    found_root = None\n",
    "    for root in candidate_roots:\n",
    "        if (root / \"notebook_context.py\").exists():\n",
    "            found_root = root\n",
    "            break\n",
    "\n",
    "    if found_root is None:\n",
    "        raise ModuleNotFoundError(\n",
    "            f\"Could not find notebook_context.py from {cwd}. \"\n",
    "            \"Try opening this notebook from inside your qepc_project folder.\"\n",
    "        )\n",
    "\n",
    "    sys.path.insert(0, str(found_root))\n",
    "    os.chdir(found_root)\n",
    "    from notebook_context import *\n",
    "\n",
    "# Fallback for project_root if notebook_context didn't define it\n",
    "try:\n",
    "    project_root\n",
    "except NameError:\n",
    "    project_root = Path.cwd()\n",
    "\n",
    "print(\"Project root:\", project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9726a-b00f-4eb2-832a-76a98c2a1852",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c12044-1e6b-484c-8dfd-fcb271ae7502",
   "metadata": {},
   "source": [
    "## üìÖ 2. Select Backtest Range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aababd5f-56e4-47b6-9490-0881364834f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtest date range: 2024-11-24 ‚Üí 2025-11-27\n"
     ]
    }
   ],
   "source": [
    "# 2. Backtest Date Range & Filters (Custom start ‚Üí today, no widgets)\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "# üëá Edit this line when you want a different start date\n",
    "BACKTEST_START_DATE = date(2024, 11, 24)\n",
    "\n",
    "# End date is always \"today\"\n",
    "BACKTEST_END_DATE = date.today()\n",
    "\n",
    "print(\"Backtest date range:\",\n",
    "      BACKTEST_START_DATE.isoformat(), \"‚Üí\", BACKTEST_END_DATE.isoformat())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f58ea-74fd-4e28-bae7-b6663206741b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ebeef-31d8-4bcd-a7a7-ae4dacf767e7",
   "metadata": {},
   "source": [
    "## üßÆ 3. Initiate Backtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593dabeb-12ab-4d23-bfa1-cdc48dd28d4e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running QEPC backtest from 2024-11-24 to 2025-11-27\n",
      "\n",
      "üöÄ STARTING LONG-RANGE BACKTEST (2024-11-24 to 2025-11-27)\n",
      "Processing... (This will update in place)\n",
      "‚è≥ Processing Day 313/369: 2025-10-02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wdors\\qepc_project\\qepc\\sports\\nba\\strengths_v2.py:233: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  team_stats[\"gameDate\"] = pd.to_datetime(team_stats[\"gameDate\"], errors=\"coerce\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No games found before the cutoff date!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Running QEPC backtest from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# The engine handles everything internally:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#   - Loads game results from Team_Stats.csv\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#   - Calculates team strengths for each day (time-travel)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#   - Runs simulations and scores predictions\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m backtest_long \u001b[38;5;241m=\u001b[39m run_season_backtest(start_date_str, end_date_str)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Games simulated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(backtest_long)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m display(backtest_long\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\qepc_project\\qepc\\backtest\\backtest_engine.py:98\u001b[0m, in \u001b[0;36mrun_season_backtest\u001b[1;34m(start_date, end_date)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚è≥ Processing Day \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_days\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Run daily test SILENTLY (verbose=False)\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m daily_df \u001b[38;5;241m=\u001b[39m run_daily_backtest(date_str, num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daily_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    101\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend(daily_df)\n",
      "File \u001b[1;32m~\\qepc_project\\qepc\\backtest\\backtest_engine.py:47\u001b[0m, in \u001b[0;36mrun_daily_backtest\u001b[1;34m(target_date, num_trials, verbose)\u001b[0m\n\u001b[0;32m     43\u001b[0m sim_schedule[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual_Away_Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m daily_schedule[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopponentScore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 3. Time Travel: Calculate Strengths\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Run silently\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m strengths \u001b[38;5;241m=\u001b[39m calculate_advanced_strengths(cutoff_date\u001b[38;5;241m=\u001b[39mtarget_date, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strengths\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[1;32m~\\qepc_project\\qepc\\sports\\nba\\strengths_v2.py:242\u001b[0m, in \u001b[0;36mcalculate_advanced_strengths\u001b[1;34m(cutoff_date, verbose)\u001b[0m\n\u001b[0;32m    239\u001b[0m team_stats \u001b[38;5;241m=\u001b[39m _apply_recency_weights(team_stats, reference_date)\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m team_stats\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo games found before the cutoff date!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# Calculate REAL stats with weighting\u001b[39;00m\n\u001b[0;32m    245\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: No games found before the cutoff date!"
     ]
    }
   ],
   "source": [
    "# 3. Run Backtest for Selected Range\n",
    "\n",
    "from qepc.backtest.backtest_engine import run_season_backtest\n",
    "\n",
    "# Convert date objects to ISO format strings (required by the engine)\n",
    "start_date_str = BACKTEST_START_DATE.isoformat()\n",
    "end_date_str = BACKTEST_END_DATE.isoformat()\n",
    "\n",
    "print(f\"üöÄ Running QEPC backtest from {start_date_str} to {end_date_str}\\n\")\n",
    "\n",
    "# The engine handles everything internally:\n",
    "#   - Loads game results from Team_Stats.csv\n",
    "#   - Calculates team strengths for each day (time-travel)\n",
    "#   - Runs simulations and scores predictions\n",
    "backtest_long = run_season_backtest(start_date_str, end_date_str)\n",
    "\n",
    "print(f\"\\nüìä Games simulated: {len(backtest_long)}\")\n",
    "display(backtest_long.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba482be0-d876-4448-8735-ce6eb2bc002c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7653d-b0a0-4d37-975c-c928a8c3ce85",
   "metadata": {},
   "source": [
    "## üèÅ 4. Global Summary (All Games in Range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f9b19-86fd-4406-9736-06a02948d173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# 4. Global Summary (All Games in Selected Range)\n",
    "\n",
    "if \"backtest_long\" not in globals() or backtest_long.empty:\n",
    "    raise RuntimeError(\"Run Cell 3 (Run Backtest) first!\")\n",
    "\n",
    "total_games = len(backtest_long)\n",
    "accuracy_pct = backtest_long[\"Correct_Pick\"].mean() * 100\n",
    "spread_mae = backtest_long[\"Spread_Error\"].abs().mean()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üèÜ QEPC BACKTEST SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÖ Date Range: {BACKTEST_START_DATE} ‚Üí {BACKTEST_END_DATE}\")\n",
    "print(f\"üèÄ Games Simulated: {total_games}\")\n",
    "print(f\"‚úÖ Overall Accuracy: {accuracy_pct:.2f}%\")\n",
    "print(f\"üéØ Avg Spread Error (MAE): {spread_mae:.2f} points\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d68a33-3bc0-4bc5-ac1b-3e392599a469",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf3ce9-3d6c-4633-a7b1-23b9e6a4a7dd",
   "metadata": {},
   "source": [
    "## üßπ 5. Clean NBA-Only View (Filter Out Exhibitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aaf014-e5af-44f5-ba3d-f9a356f49325",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# 5. Clean NBA-Only View (Filter Out Exhibitions)\n",
    "\n",
    "NBA_TEAMS = [\n",
    "    \"Atlanta Hawks\", \"Boston Celtics\", \"Brooklyn Nets\", \"Charlotte Hornets\",\n",
    "    \"Chicago Bulls\", \"Cleveland Cavaliers\", \"Dallas Mavericks\", \"Denver Nuggets\",\n",
    "    \"Detroit Pistons\", \"Golden State Warriors\", \"Houston Rockets\", \"Indiana Pacers\",\n",
    "    \"Los Angeles Clippers\", \"Los Angeles Lakers\", \"Memphis Grizzlies\", \"Miami Heat\",\n",
    "    \"Milwaukee Bucks\", \"Minnesota Timberwolves\", \"New Orleans Pelicans\", \"New York Knicks\",\n",
    "    \"Oklahoma City Thunder\", \"Orlando Magic\", \"Philadelphia 76ers\", \"Phoenix Suns\",\n",
    "    \"Portland Trail Blazers\", \"Sacramento Kings\", \"San Antonio Spurs\", \"Toronto Raptors\",\n",
    "    \"Utah Jazz\", \"Washington Wizards\",\n",
    "]\n",
    "\n",
    "backtest_clean = backtest_long[\n",
    "    backtest_long[\"Away Team\"].isin(NBA_TEAMS)\n",
    "    & backtest_long[\"Home Team\"].isin(NBA_TEAMS)\n",
    "].copy()\n",
    "\n",
    "print(f\"Original games: {len(backtest_long)}, After NBA-only filter: {len(backtest_clean)}\")\n",
    "\n",
    "if not backtest_clean.empty:\n",
    "    acc_clean = backtest_clean[\"Correct_Pick\"].mean() * 100\n",
    "    mae_clean = backtest_clean[\"Spread_Error\"].abs().mean()\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üèÜ CLEAN NBA-ONLY BACKTEST\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üèÄ Games Simulated: {len(backtest_clean)}\")\n",
    "    print(f\"‚úÖ Overall Accuracy: {acc_clean:.2f}%\")\n",
    "    print(f\"üéØ Avg Spread Error: {mae_clean:.2f} points\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No NBA-only games found in this range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162cdef3-9d2d-40eb-8f02-818c80a7ef1e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dab5db-8478-497d-aee5-f2dc7d176b5b",
   "metadata": {},
   "source": [
    "## üß® 6. Biggest Misses (Spread Error Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0420bf-a220-47d7-a436-2677d888ae98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# 6. Biggest Misses (Spread Error Analysis)\n",
    "\n",
    "# Use the cleaned NBA-only data if available, otherwise use all games\n",
    "df_to_analyze = backtest_clean if not backtest_clean.empty else backtest_long\n",
    "\n",
    "# Sort by absolute spread error (biggest misses first)\n",
    "df_to_analyze = df_to_analyze.copy()\n",
    "df_to_analyze[\"Abs_Spread_Error\"] = df_to_analyze[\"Spread_Error\"].abs()\n",
    "biggest_misses = df_to_analyze.nlargest(10, \"Abs_Spread_Error\")\n",
    "\n",
    "print(\"üß® TOP 10 BIGGEST MISSES (by Spread Error)\")\n",
    "print(\"=\" * 60)\n",
    "display(\n",
    "    biggest_misses[\n",
    "        [\"Date\", \"Away Team\", \"Home Team\", \"Expected_Spread\", \"Actual_Spread\", \"Spread_Error\"]\n",
    "    ].reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6553e5-73b6-4b21-a089-c55df8ce37f2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af160f-918d-4589-a93d-2b898e15d46c",
   "metadata": {},
   "source": [
    "## üì¶ 7. Spread Error Buckets (How Often & How Bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92e5bb-16c2-487d-a7de-90a1ff2e1ee3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# 7. Brier Score Analysis\n",
    "# \n",
    "# Brier Score measures how well-calibrated your probabilities are.\n",
    "# Lower is better. Perfect = 0.0, Random guessing = 0.25\n",
    "\n",
    "df_brier = backtest_clean.copy() if not backtest_clean.empty else backtest_long.copy()\n",
    "\n",
    "# For each game, compare predicted probability to actual outcome (0 or 1)\n",
    "# Brier Score = mean((predicted_prob - actual_outcome)^2)\n",
    "\n",
    "# Home win probability vs actual home win (1 if home won, 0 if away won)\n",
    "df_brier[\"Actual_Home_Win\"] = (df_brier[\"Actual_Home_Score\"] > df_brier[\"Actual_Away_Score\"]).astype(int)\n",
    "df_brier[\"Brier_Score\"] = (df_brier[\"Home_Win_Prob\"] - df_brier[\"Actual_Home_Win\"]) ** 2\n",
    "\n",
    "avg_brier = df_brier[\"Brier_Score\"].mean()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üìà BRIER SCORE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Average Brier Score: {avg_brier:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  ‚Ä¢ 0.00 = Perfect predictions\")\n",
    "print(\"  ‚Ä¢ 0.25 = Random coin flip\")\n",
    "print(\"  ‚Ä¢ Lower is better!\")\n",
    "print()\n",
    "\n",
    "if avg_brier < 0.20:\n",
    "    print(\"‚úÖ Excellent! Your model is well-calibrated.\")\n",
    "elif avg_brier < 0.22:\n",
    "    print(\"üëç Good. Your model beats random chance significantly.\")\n",
    "elif avg_brier < 0.25:\n",
    "    print(\"‚ö†Ô∏è Okay, but there's room for improvement.\")\n",
    "else:\n",
    "    print(\"‚ùå Model is performing at or below random chance.\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce39e99-8da8-4c52-b1c4-8a18e7f920ad",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5faac5-5b98-42e8-b92e-39c54172b782",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## üåå 8. Optional: Total Error & Script Classification (GRIND / BASE / CHAOS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f3829-bf46-4863-b859-f25313deffd2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa26f6e-b590-4700-9fff-c058c03ddf31",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# 8. Optional: Total Error & Script Classification (GRIND / BASE / CHAOS)\n",
    "\n",
    "df = backtest_clean.copy()\n",
    "\n",
    "# 8.1 Compute simulated and actual totals\n",
    "df[\"Sim_Total\"] = df[\"Sim_Home_Score\"] + df[\"Sim_Away_Score\"]\n",
    "df[\"Actual_Total\"] = df[\"Actual_Home_Score\"] + df[\"Actual_Away_Score\"]\n",
    "\n",
    "# Drop games where we effectively didn't simulate (Sim_Total == 0)\n",
    "before = len(df)\n",
    "df = df[df[\"Sim_Total\"] > 0]\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} games with Sim_Total == 0.\")\n",
    "print(f\"Remaining games for script analysis: {after}\")\n",
    "\n",
    "# 8.2 Total error\n",
    "df[\"Total_Error\"] = df[\"Actual_Total\"] - df[\"Sim_Total\"]\n",
    "\n",
    "# Thresholds for GRIND / BASE / CHAOS (can be tuned)\n",
    "grind_thresh = -15   # 15+ pts under model total ‚Üí GRIND\n",
    "chaos_thresh = 15    # 15+ pts over model total ‚Üí CHAOS\n",
    "\n",
    "def classify_script(row):\n",
    "    if row[\"Total_Error\"] <= grind_thresh:\n",
    "        return \"GRIND\"\n",
    "    elif row[\"Total_Error\"] >= chaos_thresh:\n",
    "        return \"CHAOS\"\n",
    "    else:\n",
    "        return \"BASE\"\n",
    "\n",
    "df[\"Script_ExPost\"] = df.apply(classify_script, axis=1)\n",
    "\n",
    "print(\"\\nSample of script labels:\")\n",
    "display(\n",
    "    df[[\"Date\", \"Away Team\", \"Home Team\", \"Sim_Total\", \"Actual_Total\", \"Total_Error\", \"Script_ExPost\"]]\n",
    "    .head()\n",
    ")\n",
    "\n",
    "# 8.3 Script-level summary\n",
    "script_summary = df.groupby(\"Script_ExPost\").agg(\n",
    "    Games=(\"Total_Error\", \"count\"),\n",
    "    Avg_Total_Error=(\"Total_Error\", \"mean\"),\n",
    "    Avg_Abs_Total_Error=(\"Total_Error\", lambda x: x.abs().mean()),\n",
    "    Avg_Spread_Error=(\"Spread_Error\", \"mean\"),\n",
    "    Avg_Abs_Spread_Error=(\"Spread_Error\", lambda x: x.abs().mean()),\n",
    ")\n",
    "\n",
    "script_summary[\"Percent\"] = (script_summary[\"Games\"] / len(df) * 100).round(2)\n",
    "\n",
    "print(\"\\nEx-post script distribution (based on totals):\")\n",
    "display(script_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36091349-86a6-4eb9-9cfb-38867b2d3bed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scrollable"
    ]
   },
   "source": [
    "## Global Lambda Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209aa9c-c6e2-40d9-a650-831b57f0f550",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === QEPC Global Lambda Calibration Export ===\n",
    "# Set this based on your backtest experiments.\n",
    "# For now you can leave it at 1.0; later you can change it to (for example) 0.97.\n",
    "\n",
    "scale_factor = 1.0  # üëà tweak this number when you want to shrink/boost totals\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "calibration = {\n",
    "    \"global_lambda_scale\": float(scale_factor),\n",
    "}\n",
    "\n",
    "calib_path = project_root / \"data\" / \"qepc_calibration.json\"\n",
    "calib_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(calib_path, \"w\") as f:\n",
    "    json.dump(calibration, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved calibration file to:\", calib_path)\n",
    "print(\"   Contents:\", calibration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849410a-fa95-47f7-8e15-f2b73ffbca99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## üì¶ 9. Export Backtest Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb50a7-068b-470b-af7b-073d7d78354a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Export Backtest Results ===\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create results folder if it doesn't exist\n",
    "results_dir = project_root / \"data\" / \"results\" / \"backtests\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"Backtest_{start_date_str}_to_{end_date_str}_{timestamp}.csv\"\n",
    "output_path = results_dir / filename\n",
    "\n",
    "# Use the cleaned data if available, otherwise use all games\n",
    "df_to_export = backtest_clean if not backtest_clean.empty else backtest_long\n",
    "\n",
    "if not df_to_export.empty:\n",
    "    # Save to CSV\n",
    "    df_to_export.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ BACKTEST RESULTS EXPORTED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÅ Location: {output_path}\")\n",
    "    print(f\"üìä Games: {len(df_to_export)}\")\n",
    "    print(f\"üìÖ Date Range: {start_date_str} to {end_date_str}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to export (empty dataframe)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
