{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ QEPC Enhanced Backtest with Real Results\n",
    "\n",
    "This notebook:\n",
    "1. Uses **actual game results** from TeamStatistics.csv\n",
    "2. Compares QEPC predictions to real outcomes\n",
    "3. Calculates detailed accuracy metrics\n",
    "4. Identifies patterns in prediction errors\n",
    "5. Generates calibration insights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_context import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# QEPC imports\n",
    "from qepc.sports.nba.strength import calculate_advanced_strengths\n",
    "from qepc.sports.nba.lambda_calc import compute_lambda\n",
    "from qepc.sports.nba.sim import run_qepc_simulation\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"üìÅ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Load Actual Game Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TeamStatistics.csv with actual results\n",
    "team_stats_path = project_root / \"data\" / \"raw\" / \"TeamStatistics.csv\"\n",
    "team_stats = pd.read_csv(team_stats_path)\n",
    "\n",
    "print(f\"üì¶ Loaded {len(team_stats):,} rows from TeamStatistics.csv\")\n",
    "\n",
    "# Parse dates\n",
    "team_stats['gameDate'] = pd.to_datetime(team_stats['gameDate'], format='mixed')\n",
    "\n",
    "# Get 2025 season games only\n",
    "games_2025 = team_stats[team_stats['gameDate'].dt.year == 2025].copy()\n",
    "\n",
    "# Create team names\n",
    "games_2025['Team'] = games_2025['teamCity'] + ' ' + games_2025['teamName']\n",
    "games_2025['Opponent'] = games_2025['opponentTeamCity'] + ' ' + games_2025['opponentTeamName']\n",
    "\n",
    "# Get only home games (to avoid duplicates)\n",
    "home_games = games_2025[games_2025['home'] == 1].copy()\n",
    "\n",
    "print(f\"\\nüìä 2025 Season:\")\n",
    "print(f\"   Date range: {games_2025['gameDate'].min().date()} to {games_2025['gameDate'].max().date()}\")\n",
    "print(f\"   Total games: {len(home_games)}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nüèÄ Recent Games (sample):\")\n",
    "sample = home_games.nlargest(5, 'gameDate')[[\n",
    "    'gameDate', 'Opponent', 'Team', 'opponentScore', 'teamScore', 'win'\n",
    "]]\n",
    "for _, game in sample.iterrows():\n",
    "    date = game['gameDate'].strftime('%Y-%m-%d')\n",
    "    score = f\"{int(game['opponentScore'])}-{int(game['teamScore'])}\"\n",
    "    result = \"W\" if game['win'] == 1 else \"L\"\n",
    "    print(f\"   {date}: {game['Opponent']} @ {game['Team']} - {score} ({result})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Set Backtest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKTEST DATE RANGE\n",
    "# Use dates you have actual results for!\n",
    "\n",
    "BACKTEST_START = pd.Timestamp(\"2025-10-22\")  # Season start\n",
    "BACKTEST_END = pd.Timestamp(\"2025-11-17\")    # Latest available data\n",
    "\n",
    "print(f\"üéØ Backtest Configuration:\")\n",
    "print(f\"   Start: {BACKTEST_START.date()}\")\n",
    "print(f\"   End:   {BACKTEST_END.date()}\")\n",
    "print(f\"   Days:  {(BACKTEST_END - BACKTEST_START).days}\")\n",
    "\n",
    "# Filter games to backtest window\n",
    "backtest_games = home_games[\n",
    "    (home_games['gameDate'] >= BACKTEST_START) &\n",
    "    (home_games['gameDate'] <= BACKTEST_END)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nüìä Games in backtest window: {len(backtest_games)}\")\n",
    "\n",
    "if len(backtest_games) == 0:\n",
    "    print(\"\\n‚ùå No games found in this date range!\")\n",
    "    print(f\"   Available range: {home_games['gameDate'].min().date()} to {home_games['gameDate'].max().date()}\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Ready to backtest!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run QEPC Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÆ Running QEPC predictions on historical games...\\n\")\n",
    "\n",
    "predictions = []\n",
    "errors = []\n",
    "\n",
    "for idx, game in backtest_games.iterrows():\n",
    "    home_team = game['Team']\n",
    "    away_team = game['Opponent']\n",
    "    game_date = game['gameDate']\n",
    "    \n",
    "    try:\n",
    "        # Calculate team strengths\n",
    "        strengths = calculate_advanced_strengths()\n",
    "        \n",
    "        # Get lambdas\n",
    "        home_lambda = compute_lambda(home_team, away_team, is_home=True, strengths=strengths)\n",
    "        away_lambda = compute_lambda(away_team, home_team, is_home=False, strengths=strengths)\n",
    "        \n",
    "        # Run simulation\n",
    "        sim_results = run_qepc_simulation(\n",
    "            home_team=home_team,\n",
    "            away_team=away_team,\n",
    "            home_lambda=home_lambda,\n",
    "            away_lambda=away_lambda,\n",
    "            n_sims=10000\n",
    "        )\n",
    "        \n",
    "        # Extract predictions\n",
    "        pred_home = sim_results['home_score_mean']\n",
    "        pred_away = sim_results['away_score_mean']\n",
    "        \n",
    "        # Actual scores\n",
    "        actual_home = game['teamScore']\n",
    "        actual_away = game['opponentScore']\n",
    "        \n",
    "        # Store result\n",
    "        predictions.append({\n",
    "            'Date': game_date,\n",
    "            'Home_Team': home_team,\n",
    "            'Away_Team': away_team,\n",
    "            \n",
    "            # Predictions\n",
    "            'Pred_Home_Score': pred_home,\n",
    "            'Pred_Away_Score': pred_away,\n",
    "            'Pred_Total': pred_home + pred_away,\n",
    "            'Pred_Spread': pred_home - pred_away,\n",
    "            'Pred_Winner': home_team if pred_home > pred_away else away_team,\n",
    "            \n",
    "            # Actuals\n",
    "            'Actual_Home_Score': actual_home,\n",
    "            'Actual_Away_Score': actual_away,\n",
    "            'Actual_Total': actual_home + actual_away,\n",
    "            'Actual_Spread': actual_home - actual_away,\n",
    "            'Actual_Winner': home_team if game['win'] == 1 else away_team,\n",
    "            \n",
    "            # Errors\n",
    "            'Error_Home': abs(pred_home - actual_home),\n",
    "            'Error_Away': abs(pred_away - actual_away),\n",
    "            'Error_Total': abs((pred_home + pred_away) - (actual_home + actual_away)),\n",
    "            'Error_Spread': abs((pred_home - pred_away) - (actual_home - actual_away)),\n",
    "            \n",
    "            # Correct?\n",
    "            'Winner_Correct': (pred_home > pred_away) == (game['win'] == 1),\n",
    "            \n",
    "            # Confidence metrics\n",
    "            'Home_Win_Prob': sim_results.get('home_win_prob', 0.5),\n",
    "            'Confidence': abs(pred_home - pred_away)\n",
    "        })\n",
    "        \n",
    "        # Progress\n",
    "        if len(predictions) % 10 == 0:\n",
    "            print(f\"   Processed {len(predictions)}/{len(backtest_games)} games...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors.append({\n",
    "            'Date': game_date,\n",
    "            'Home_Team': home_team,\n",
    "            'Away_Team': away_team,\n",
    "            'Error': str(e)\n",
    "        })\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(predictions)\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions complete!\")\n",
    "print(f\"   Successful: {len(predictions)}\")\n",
    "print(f\"   Errors: {len(errors)}\")\n",
    "\n",
    "if len(errors) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Errors encountered:\")\n",
    "    for err in errors[:5]:\n",
    "        print(f\"   {err['Date'].date()}: {err['Away_Team']} @ {err['Home_Team']} - {err['Error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Calculate Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä QEPC BACKTEST RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Win prediction accuracy\n",
    "    win_accuracy = results_df['Winner_Correct'].mean()\n",
    "    print(f\"\\nüéØ Win Prediction Accuracy: {win_accuracy:.1%}\")\n",
    "    print(f\"   Correct: {results_df['Winner_Correct'].sum()}/{len(results_df)}\")\n",
    "    \n",
    "    # Score accuracy\n",
    "    print(f\"\\nüìä Score Prediction Accuracy:\")\n",
    "    print(f\"   Avg Home Error:   {results_df['Error_Home'].mean():.2f} points\")\n",
    "    print(f\"   Avg Away Error:   {results_df['Error_Away'].mean():.2f} points\")\n",
    "    print(f\"   Avg Total Error:  {results_df['Error_Total'].mean():.2f} points\")\n",
    "    print(f\"   Avg Spread Error: {results_df['Error_Spread'].mean():.2f} points\")\n",
    "    \n",
    "    # Error distribution\n",
    "    print(f\"\\nüìâ Error Distribution:\")\n",
    "    print(f\"   Total Error Median: {results_df['Error_Total'].median():.2f}\")\n",
    "    print(f\"   Total Error Std:    {results_df['Error_Total'].std():.2f}\")\n",
    "    print(f\"   Total Error Max:    {results_df['Error_Total'].max():.2f}\")\n",
    "    \n",
    "    # Accuracy by confidence\n",
    "    high_conf = results_df[results_df['Confidence'] >= 5]\n",
    "    if len(high_conf) > 0:\n",
    "        print(f\"\\nüéØ High Confidence Games (spread >= 5):\")\n",
    "        print(f\"   Count: {len(high_conf)}\")\n",
    "        print(f\"   Win Accuracy: {high_conf['Winner_Correct'].mean():.1%}\")\n",
    "    \n",
    "    # Best predictions\n",
    "    print(f\"\\nüèÜ Best Predictions (smallest total error):\")\n",
    "    best = results_df.nsmallest(5, 'Error_Total')[[\n",
    "        'Date', 'Away_Team', 'Home_Team', \n",
    "        'Pred_Total', 'Actual_Total', 'Error_Total'\n",
    "    ]]\n",
    "    for _, row in best.iterrows():\n",
    "        print(f\"   {row['Date'].date()}: {row['Away_Team']} @ {row['Home_Team']}\")\n",
    "        print(f\"      Predicted: {row['Pred_Total']:.1f} | Actual: {row['Actual_Total']:.0f} | Error: {row['Error_Total']:.1f}\")\n",
    "    \n",
    "    # Worst predictions\n",
    "    print(f\"\\n‚ö†Ô∏è  Worst Predictions (largest total error):\")\n",
    "    worst = results_df.nlargest(5, 'Error_Total')[[\n",
    "        'Date', 'Away_Team', 'Home_Team', \n",
    "        'Pred_Total', 'Actual_Total', 'Error_Total'\n",
    "    ]]\n",
    "    for _, row in worst.iterrows():\n",
    "        print(f\"   {row['Date'].date()}: {row['Away_Team']} @ {row['Home_Team']}\")\n",
    "        print(f\"      Predicted: {row['Pred_Total']:.1f} | Actual: {row['Actual_Total']:.0f} | Error: {row['Error_Total']:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\nelse:\n",
    "    print(\"‚ùå No predictions to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Predicted vs Actual Total\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(results_df['Actual_Total'], results_df['Pred_Total'], alpha=0.6)\n",
    "    ax1.plot([200, 250], [200, 250], 'r--', label='Perfect prediction')\n",
    "    ax1.set_xlabel('Actual Total')\n",
    "    ax1.set_ylabel('Predicted Total')\n",
    "    ax1.set_title('Predicted vs Actual Total Score')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Error Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(results_df['Error_Total'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(results_df['Error_Total'].mean(), color='r', linestyle='--', label='Mean')\n",
    "    ax2.set_xlabel('Total Error (points)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Total Score Error')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Error Over Time\n",
    "    ax3 = axes[1, 0]\n",
    "    results_df_sorted = results_df.sort_values('Date')\n",
    "    ax3.plot(results_df_sorted['Date'], results_df_sorted['Error_Total'], marker='o', alpha=0.6)\n",
    "    ax3.axhline(results_df['Error_Total'].mean(), color='r', linestyle='--', label='Mean Error')\n",
    "    ax3.set_xlabel('Date')\n",
    "    ax3.set_ylabel('Total Error (points)')\n",
    "    ax3.set_title('Error Over Time')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Win Accuracy by Confidence\n",
    "    ax4 = axes[1, 1]\n",
    "    confidence_bins = pd.cut(results_df['Confidence'], bins=[0, 3, 6, 9, 100])\n",
    "    accuracy_by_conf = results_df.groupby(confidence_bins)['Winner_Correct'].mean()\n",
    "    accuracy_by_conf.plot(kind='bar', ax=ax4, color='steelblue')\n",
    "    ax4.axhline(0.5, color='r', linestyle='--', label='Coin flip')\n",
    "    ax4.set_xlabel('Confidence (Predicted Spread)')\n",
    "    ax4.set_ylabel('Win Accuracy')\n",
    "    ax4.set_title('Win Accuracy by Confidence Level')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations complete\")\n",
    "else:\n",
    "    print(\"‚ùå No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df) > 0:\n",
    "    # Save detailed results\n",
    "    output_dir = project_root / \"data\" / \"results\" / \"backtests\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    filename = f\"Enhanced_Backtest_{BACKTEST_START.date()}_to_{BACKTEST_END.date()}.csv\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Saved detailed results to: {output_path}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'Backtest_Period': f\"{BACKTEST_START.date()} to {BACKTEST_END.date()}\",\n",
    "        'Games_Analyzed': len(results_df),\n",
    "        'Win_Accuracy': f\"{win_accuracy:.1%}\",\n",
    "        'Avg_Total_Error': f\"{results_df['Error_Total'].mean():.2f}\",\n",
    "        'Avg_Spread_Error': f\"{results_df['Error_Spread'].mean():.2f}\",\n",
    "        'Median_Total_Error': f\"{results_df['Error_Total'].median():.2f}\"\n",
    "    }\n",
    "    \n",
    "    summary_path = output_dir / f\"Summary_{BACKTEST_START.date()}_to_{BACKTEST_END.date()}.txt\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        for key, value in summary.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"üíæ Saved summary to: {summary_path}\")\n",
    "    print(\"\\n‚úÖ All results saved!\")\n",
    "else:\n",
    "    print(\"‚ùå No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "### Improve QEPC Based on Results:\n",
    "\n",
    "1. **If Win Accuracy < 55%:**\n",
    "   - Adjust lambda calculations\n",
    "   - Add recency weighting\n",
    "   - Include rest day adjustments\n",
    "\n",
    "2. **If Total Error > 15 points:**\n",
    "   - Calibrate offensive/defensive ratings\n",
    "   - Add pace adjustments\n",
    "   - Consider team form metrics\n",
    "\n",
    "3. **If High Confidence Games Underperform:**\n",
    "   - Review spread calculations\n",
    "   - Add upset probability\n",
    "   - Consider situational factors\n",
    "\n",
    "### Additional Backtests:\n",
    "\n",
    "- Split by home/away\n",
    "- Analyze by team\n",
    "- Test on different date ranges\n",
    "- Compare to betting lines\n",
    "\n",
    "### Integration:\n",
    "\n",
    "- Use results to calibrate QEPC\n",
    "- Build confidence intervals\n",
    "- Create ensemble models\n",
    "- Develop betting strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Your backtest is complete! Use these insights to improve QEPC.** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
