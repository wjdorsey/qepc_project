{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4a3d3-aefd-4219-8623-3d4217abdaf5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# üßæ QEPC Project Backup (Fast / Filtered)\n",
    "#\n",
    "# This backup:\n",
    "#   - Detects the project root (folder that has both data/ and notebooks/)\n",
    "#   - Creates backups/<qepc_backup_YYYYMMDD_HHMMSS>.zip\n",
    "#   - INCLUDES: code, notebooks, configs, light data\n",
    "#   - SKIPS:\n",
    "#       * data/raw (huge CSVs you can regenerate)\n",
    "#       * notebooks/02_utilities/data/raw\n",
    "#       * backups/ (older backups)\n",
    "#       * .git, __pycache__, .ipynb_checkpoints\n",
    "#       * any single file larger than MAX_FILE_MB\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Detect project root\n",
    "# ----------------------------\n",
    "here = Path.cwd().resolve()\n",
    "project_root = None\n",
    "\n",
    "for p in [here] + list(here.parents):\n",
    "    if (p / \"data\").exists() and (p / \"notebooks\").exists():\n",
    "        project_root = p\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    raise RuntimeError(\n",
    "        \"Could not detect project root. \"\n",
    "        \"Open this notebook from inside C:/Users/wdors/qepc_project or a subfolder.\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Project root detected: {project_root}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Backup config\n",
    "# ----------------------------\n",
    "\n",
    "backups_dir = project_root / \"backups\"\n",
    "backups_dir.mkdir(exist_ok=True)\n",
    "print(f\"üìÅ Backups directory: {backups_dir}\")\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_name = f\"qepc_backup_{timestamp}.zip\"\n",
    "backup_path = backups_dir / backup_name\n",
    "\n",
    "# Maximum single-file size to include (in MB)\n",
    "MAX_FILE_MB = 60\n",
    "MAX_FILE_BYTES = MAX_FILE_MB * 1024 * 1024\n",
    "\n",
    "print(f\"üóÇÔ∏è Backup file will be created as: {backup_path}\")\n",
    "print(f\"‚öôÔ∏è Max single-file size: {MAX_FILE_MB} MB\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Include / exclude rules\n",
    "# ----------------------------\n",
    "\n",
    "def should_exclude(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if this path should be excluded from the backup.\n",
    "    Decisions are based on the path *relative* to project_root.\n",
    "    \"\"\"\n",
    "    rel = path.relative_to(project_root)\n",
    "    parts = rel.parts\n",
    "\n",
    "    # Skip backup folder itself\n",
    "    if parts[0] == \"backups\":\n",
    "        return True\n",
    "\n",
    "    # Skip git metadata / caches / checkpoints\n",
    "    if parts[0] in {\".git\", \"__pycache__\"}:\n",
    "        return True\n",
    "    if \".ipynb_checkpoints\" in parts:\n",
    "        return True\n",
    "\n",
    "    # Skip virtualenvs if any live inside the project\n",
    "    if parts[0] in {\"venv\", \".venv\", \"env\", \".env\"}:\n",
    "        return True\n",
    "\n",
    "    # Skip heavy raw data ‚Äì these are regenerable\n",
    "    #  - data/raw/...\n",
    "    if len(parts) >= 2 and parts[0] == \"data\" and parts[1] == \"raw\":\n",
    "        return True\n",
    "\n",
    "    #  - notebooks/02_utilities/data/raw/...\n",
    "    if (\n",
    "        len(parts) >= 4\n",
    "        and parts[0] == \"notebooks\"\n",
    "        and parts[1] == \"02_utilities\"\n",
    "        and parts[2] == \"data\"\n",
    "        and parts[3] == \"raw\"\n",
    "    ):\n",
    "        return True\n",
    "\n",
    "    # Skip existing zip/tar archives inside project (other than the one we're making)\n",
    "    if rel.suffix in {\".zip\", \".tar\", \".gz\", \".bz2\"} and parts[0] != \"data\":\n",
    "        # You can adjust this rule if needed\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Collect files to back up\n",
    "# ----------------------------\n",
    "\n",
    "files_to_add = []\n",
    "skipped_large = []\n",
    "skipped_rules = []\n",
    "\n",
    "for path in project_root.rglob(\"*\"):\n",
    "    if not path.is_file():\n",
    "        continue\n",
    "\n",
    "    if should_exclude(path):\n",
    "        skipped_rules.append(path)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        size = path.stat().st_size\n",
    "    except OSError:\n",
    "        continue\n",
    "\n",
    "    if size > MAX_FILE_BYTES:\n",
    "        skipped_large.append((path, size))\n",
    "        continue\n",
    "\n",
    "    files_to_add.append((path, size))\n",
    "\n",
    "total_size_bytes = sum(size for _, size in files_to_add)\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"üì¶ Files to include: {len(files_to_add)}\")\n",
    "print(f\"üì¶ Approx compressed input size: {total_size_mb:.2f} MB\")\n",
    "print(f\"üö´ Skipped by rules: {len(skipped_rules)} files\")\n",
    "print(f\"üö´ Skipped as too large (> {MAX_FILE_MB} MB): {len(skipped_large)} files\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Create the ZIP\n",
    "# ----------------------------\n",
    "\n",
    "with zipfile.ZipFile(backup_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for idx, (path, size) in enumerate(files_to_add, start=1):\n",
    "        rel = path.relative_to(project_root)\n",
    "        zf.write(path, rel)\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"  ‚Üí Added {idx} files so far...\")\n",
    "\n",
    "print(\"\\n‚úÖ Backup complete!\")\n",
    "final_size_mb = backup_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"üßæ Backup file: {backup_path.name} ({final_size_mb:.2f} MB)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Show what was skipped (summary)\n",
    "# ----------------------------\n",
    "\n",
    "if skipped_large:\n",
    "    print(\"\\n‚ö†Ô∏è The following large files were skipped (size > \"\n",
    "          f\"{MAX_FILE_MB} MB):\")\n",
    "    for path, size in skipped_large:\n",
    "        print(f\"  - {path.relative_to(project_root)}  ({size / (1024*1024):.2f} MB)\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb2231-583c-4442-b106-13716c185a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\wdors\\qepc_project\")\n",
    "\n",
    "from qepc.backtest.backtest_engine import run_daily_backtest\n",
    "\n",
    "# Try backtest with verbose=True to see what's happening\n",
    "result = run_daily_backtest(\"2025-01-15\", num_trials=100, verbose=True)\n",
    "\n",
    "if not result.empty:\n",
    "    print(\"\\n‚úÖ Backtest successful!\")\n",
    "    print(result.head())\n",
    "else:\n",
    "    print(\"‚ùå No data for that date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdc9e0-e58b-4220-875f-33a4a9b006b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qepc.notebook_header import qepc_notebook_setup\n",
    "env = qepc_notebook_setup(run_diagnostics=False)\n",
    "\n",
    "from qepc.backtest.backtest_engine import run_daily_backtest\n",
    "\n",
    "# Try a backtest for a date you have data for\n",
    "result = run_daily_backtest(\"2025-11-22\", num_trials=1000, verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc235957-71fd-4efc-aac2-409b47496456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add your project to Python path\n",
    "project_path = r\"C:\\Users\\wdors\\qepc_project\"\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "# Check if files exist\n",
    "print(\"Checking file structure...\")\n",
    "print(f\"\\nProject root: {project_path}\")\n",
    "print(f\"Exists: {os.path.exists(project_path)}\")\n",
    "\n",
    "# Check for qepc folder\n",
    "qepc_path = os.path.join(project_path, \"qepc\")\n",
    "print(f\"\\nqepc folder: {qepc_path}\")\n",
    "print(f\"Exists: {os.path.exists(qepc_path)}\")\n",
    "\n",
    "# Check for __init__.py\n",
    "init_path = os.path.join(qepc_path, \"__init__.py\")\n",
    "print(f\"\\nqepc/__init__.py: {init_path}\")\n",
    "print(f\"Exists: {os.path.exists(init_path)}\")\n",
    "\n",
    "# Check for notebook_header.py\n",
    "nb_header_path = os.path.join(project_path, \"notebook_header.py\")\n",
    "print(f\"\\nnotebook_header.py: {nb_header_path}\")\n",
    "print(f\"Exists: {os.path.exists(nb_header_path)}\")\n",
    "\n",
    "# List what's in the project root\n",
    "print(f\"\\nFiles in project root:\")\n",
    "for item in os.listdir(project_path):\n",
    "    item_path = os.path.join(project_path, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        print(f\"  üìÅ {item}/\")\n",
    "    else:\n",
    "        print(f\"  üìÑ {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504d582-c390-46bf-9901-7c262b367b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to get the true project root from QEPC's autoload paths module\n",
    "try:\n",
    "    from qepc.autoload.paths import get_project_root\n",
    "    project_root = get_project_root()\n",
    "except Exception:\n",
    "    # Fallback if that import fails for some reason\n",
    "    project_root = Path.cwd()\n",
    "    print(\"‚ö†Ô∏è Falling back to cwd as project root\")\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "# Helper: pick the \"best\" match for a file name among many\n",
    "def pick_best_match(matches):\n",
    "    if not matches:\n",
    "        return None\n",
    "    # Prefer paths that live under a 'data' folder and NOT under 'notebooks'\n",
    "    scored = []\n",
    "    for p in matches:\n",
    "        score = 0\n",
    "        parts = [str(part).lower() for part in p.parts]\n",
    "        if \"data\" in parts:\n",
    "            score += 2\n",
    "        if \"raw\" in parts:\n",
    "            score += 1\n",
    "        if \"props\" in parts:\n",
    "            score += 1\n",
    "        if \"results\" in parts:\n",
    "            score += 1\n",
    "        if \"notebooks\" in parts:\n",
    "            score -= 2\n",
    "        if \".ipynb_checkpoints\" in str(p):\n",
    "            score -= 5\n",
    "        scored.append((score, p))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return scored[0][1]\n",
    "\n",
    "# (label, filename)\n",
    "targets = [\n",
    "    # Core game/team data\n",
    "    (\"TeamStatistics (team game logs)\",      \"TeamStatistics.csv\"),\n",
    "    (\"Team_Stats (team season stats)\",       \"Team_Stats.csv\"),\n",
    "    (\"PlayerStatistics (player logs)\",       \"PlayerStatistics.csv\"),\n",
    "    (\"Canonical Games (schedule)\",           \"Games.csv\"),\n",
    "    (\"GameResults_2025 (results)\",           \"GameResults_2025.csv\"),\n",
    "    (\"Schedule_with_Rest\",                   \"Schedule_with_Rest.csv\"),\n",
    "    (\"TeamForm\",                             \"TeamForm.csv\"),\n",
    "\n",
    "    # Roster / players\n",
    "    (\"Players\",                              \"Players.csv\"),\n",
    "    (\"Players_Processed\",                    \"Players_Processed.csv\"),\n",
    "\n",
    "    # Injuries\n",
    "    (\"Injury_Overrides\",                     \"Injury_Overrides.csv\"),\n",
    "    (\"Injury_Overrides_MASTER\",              \"Injury_Overrides_MASTER.csv\"),\n",
    "    (\"Injury_Overrides_live_espn\",           \"Injury_Overrides_live_espn.csv\"),\n",
    "\n",
    "    # Props / aggregates\n",
    "    (\"Player_Season_Averages\",               \"Player_Season_Averages.csv\"),\n",
    "    (\"Player_Averages_With_CI\",              \"Player_Averages_With_CI.csv\"),\n",
    "    (\"Player_Recent_Form_L5\",                \"Player_Recent_Form_L5.csv\"),\n",
    "    (\"Player_Recent_Form_L10\",               \"Player_Recent_Form_L10.csv\"),\n",
    "    (\"Player_Recent_Form_L15\",               \"Player_Recent_Form_L15.csv\"),\n",
    "    (\"Player_Home_Away_Splits\",              \"Player_Home_Away_Splits.csv\"),\n",
    "]\n",
    "\n",
    "def preview_by_filename(label: str, filename: str, n: int = 3):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìÑ {label}\")\n",
    "    print(f\"Looking for filename: {filename}\")\n",
    "\n",
    "    # Find all matches anywhere under project_root\n",
    "    matches = [p for p in project_root.rglob(filename)]\n",
    "    if not matches:\n",
    "        print(\"‚ö†Ô∏è No matches found in project.\")\n",
    "        return\n",
    "\n",
    "    print(\"Found matches:\")\n",
    "    for m in matches:\n",
    "        try:\n",
    "            rel = m.relative_to(project_root)\n",
    "        except ValueError:\n",
    "            rel = m\n",
    "        print(\"   ‚Ä¢\", rel)\n",
    "\n",
    "    best = pick_best_match(matches)\n",
    "    if best is None:\n",
    "        print(\"‚ö†Ô∏è Could not choose a best match.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        rel_best = best.relative_to(project_root)\n",
    "    except ValueError:\n",
    "        rel_best = best\n",
    "\n",
    "    print(f\"\\n‚úÖ Using best match: {rel_best}\")\n",
    "\n",
    "    # Load a small sample (nrows=3) to avoid pulling full 300MB files\n",
    "    try:\n",
    "        df_sample = pd.read_csv(best, nrows=n)\n",
    "        print(f\"Sample shape: {df_sample.shape}\")\n",
    "        print(\"Columns:\", list(df_sample.columns))\n",
    "        print(\"\\nSample rows:\")\n",
    "        display(df_sample)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading CSV sample: {e}\")\n",
    "\n",
    "for label, filename in targets:\n",
    "    preview_by_filename(label, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de750c5b-f5a2-4560-be1a-7f1adb118608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\wdors\\qepc_project\")\n",
    "\n",
    "# Clear cache\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if 'qepc' in mod:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "from qepc.backtest.backtest_engine import run_daily_backtest\n",
    "\n",
    "# Try a date from your data - based on your samples, you have games around 2025-11-17\n",
    "result = run_daily_backtest(\"2025-11-17\", num_trials=100, verbose=True)\n",
    "\n",
    "if not result.empty:\n",
    "    print(\"\\n‚úÖ Success!\")\n",
    "    print(result.head())\n",
    "else:\n",
    "    print(\"No data found - might need a different date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6510259-6278-42c8-b6ee-bdd9953591db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different dates from your data\n",
    "result = run_daily_backtest(\"2025-11-16\", num_trials=100, verbose=True)\n",
    "print(f\"Games found: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe916f-64a7-4cc9-b01e-4d697beea1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_result = run_season_backtest(\"2024-11-15\", \"2025-11-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece749e-0fa9-473a-8ef0-b996a1fb2ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
